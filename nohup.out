INFO:root:loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 99
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
INFO:root:torch.Size([64, 4, 16, 768])
INFO:root:torch.Size([4, 16, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:37.626403
INFO:root:Total pretraining time 0:01:37.626513
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 199 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 199 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 199 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 199
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 18, 768])
INFO:root:torch.Size([64, 4, 20, 768])
INFO:root:torch.Size([4, 20, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:37.387526
INFO:root:Total pretraining time 0:01:37.387657
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 299
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 23, 768])
INFO:root:torch.Size([64, 4, 16, 768])
INFO:root:torch.Size([4, 16, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:36.915691
INFO:root:Total pretraining time 0:01:36.915819
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 399 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 399 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 399 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 399
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 25, 768])
INFO:root:torch.Size([64, 4, 16, 768])
INFO:root:torch.Size([4, 16, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:36.355723
INFO:root:Total pretraining time 0:01:36.355883
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 499 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 499 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 499 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 499
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 100, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
CRITICAL:root:x, in ViT has shape: torch.Size([64, 19, 768])
INFO:root:torch.Size([64, 4, 20, 768])
INFO:root:torch.Size([4, 20, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:38.553927
INFO:root:Total pretraining time 0:01:38.554078
INFO:root:called-params configs/iic-eval.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 150,
                'image_folder': 'intel-image-classification/',
                'num_workers': 0,
                'pin_mem': True,
                'root_path': 'datasets',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 100,
                   'folder': 'logs_PKT/iic-train-L2-seed-10/',
                   'logging_frequency': 1,
                   'output_file': 'oiic-evaluate-L2-seed10-matrices.out',
                   'plot_matrices': False,
                   'tensorboard_dir': 'eval_tb/',
                   'use_tensorboard': True,
                   'write_tag': 'jepa_iic_PKT_seed-0-ep____.pth.tar'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 1,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 15,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_base',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': 'jepa_iic_L2_seed-10',
                'use_bfloat16': False},
    'optimization': {   'ema': [0.996, 1.0],
                        'epochs': 500,
                        'evaluate': True,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'PKT',
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 40,
                        'weight_decay': 0.04}}
INFO:root:Running... (rank: 0/1)
CRITICAL:root:EVALUATING
INFO:root:tarfiles: ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 99
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
INFO:root:torch.Size([64, 4, 16, 768])
INFO:root:torch.Size([4, 16, 768])
INFO:root:Iteration: 1
INFO:root:called-params configs/iic-eval.yaml
INFO:root:loaded params...
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:40112 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 150,
                'image_folder': 'intel-image-classification/',
                'num_workers': 0,
                'pin_mem': True,
                'root_path': 'datasets',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 100,
                   'folder': 'logs_PKT/iic-train-PKT-seed-10/',
                   'logging_frequency': 1,
                   'output_file': 'oiic-evaluate-PKT-seed10-matrices.out',
                   'plot_matrices': False,
                   'tensorboard_dir': 'eval_tb/',
                   'use_tensorboard': True,
                   'write_tag': 'jepa_iic_PKT_seed-0-ep____.pth.tar'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 1,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 15,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_base',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': 'jepa_iic_PKT',
                'use_bfloat16': False},
    'optimization': {   'ema': [0.996, 1.0],
                        'epochs': 500,
                        'evaluate': True,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'PKT',
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 40,
                        'weight_decay': 0.04}}
INFO:root:distributed training not available The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:40112 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
INFO:root:Running... (rank: 0/1)
CRITICAL:root:EVALUATING
INFO:root:tarfiles: ['logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep400.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep300.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep100.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep200.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep500.pth.tar']
INFO:root:working on file logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep100.pth.tar out of ['logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep400.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep300.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep100.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep200.pth.tar', 'logs_PKT/iic-train-PKT-seed-10/jepa_iic_PKT-ep500.pth.tar']...
INFO:root:SLURM vars not set (distributed training not available)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
Process Process-1:
Traceback (most recent call last):
  File "/media/data/lazarosg/miniconda3/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/media/data/lazarosg/miniconda3/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/media/data/lazarosg/ijepa/main.py", line 98, in process_main
    evall(args=temp_params)
  File "/media/data/lazarosg/ijepa/src/eval.py", line 256, in main
    encoder = DistributedDataParallel(encoder, static_graph=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/data/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 731, in __init__
    self.process_group = _get_default_group()
                         ^^^^^^^^^^^^^^^^^^^^
  File "/media/data/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 1008, in _get_default_group
    raise ValueError(
ValueError: Default process group has not been initialized, please make sure to call init_process_group.
INFO:root:time taken for epoch 0:01:39.108074
INFO:root:Total pretraining time 0:01:39.108192
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 199 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 199 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 199 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 199
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
INFO:root:torch.Size([64, 4, 20, 768])
INFO:root:torch.Size([4, 20, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:37.565480
INFO:root:Total pretraining time 0:01:37.565592
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 299
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
INFO:root:torch.Size([64, 4, 16, 768])
INFO:root:torch.Size([4, 16, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:37.658913
INFO:root:Total pretraining time 0:01:37.659036
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 399 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 399 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 399 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 399
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
INFO:root:torch.Size([64, 4, 16, 768])
INFO:root:torch.Size([4, 16, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:37.483256
INFO:root:Total pretraining time 0:01:37.483382
INFO:root:working on file logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar out of ['logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep100.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep200.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep300.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep400.pth.tar', 'logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar']...
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/intel-image-classification/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 499 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 499 with msg: <All keys matched successfully>
['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 499 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 499
INFO:root:read-path: logs_PKT/iic-train-L2-seed-10/jepa_iic_L2_seed-10-ep500.pth.tar
INFO:root:Starting
INFO:root:Iteration: 0
INFO:root:torch.Size([64, 4, 20, 768])
INFO:root:torch.Size([4, 20, 768])
INFO:root:Iteration: 1
INFO:root:time taken for epoch 0:01:37.041507
INFO:root:Total pretraining time 0:01:37.041657
