VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
loss inside PKT is : 1.420443e-05
loss inside PKT is : 1.269776e-05
loss inside PKT is : 1.467686e-05
loss inside PKT is : 1.376526e-05
loss inside PKT is : 1.460483e-05
loss inside PKT is : 1.168173e-05
loss inside PKT is : 1.688334e-05
loss inside PKT is : 1.678543e-05
loss inside PKT is : 1.628245e-05
loss inside PKT is : 1.618475e-05
loss inside PKT is : 1.144484e-05
loss inside PKT is : 1.499097e-05
loss inside PKT is : 1.556731e-05
loss inside PKT is : 1.539774e-05
loss inside PKT is : 1.391201e-05
loss inside PKT is : 1.553930e-05
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
1.0e0
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/stl10/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
Epoch 1
[1,     0] loss: 2.429885e-02 masks: 15.0 20.0 [wd: 4.00e-02] [lr: 2.00e-04] [mem: 8.39e+03] (910.1 ms)
[1,     0] grad_stats: [1.20e-03 5.64e-04] (5.59e-04, 1.35e-03)
[1,   390] loss: 1.161699e-02 masks: 20.0 17.4 [wd: 4.00e-02] [lr: 2.67e-04] [mem: 1.28e+04] (432.9 ms)
[1,   390] grad_stats: [1.04e-03 3.24e-05] (3.24e-05, 1.11e-03)
[1,   780] loss: 9.475153e-03 masks: 20.2 17.2 [wd: 4.02e-02] [lr: 3.33e-04] [mem: 1.30e+04] (435.1 ms)
[1,   780] grad_stats: [1.42e-03 5.18e-05] (2.90e-05, 1.89e-03)
avg. loss 9.47515262e-03
time taken for epoch 0:05:52.531033
Epoch 2
[2,     0] loss: 5.180479e-03 masks: 33.0 12.0 [wd: 4.02e-02] [lr: 3.34e-04] [mem: 1.30e+04] (504.3 ms)
[2,     0] grad_stats: [8.55e-04 3.60e-05] (2.90e-05, 1.10e-03)
[2,   390] loss: 4.079045e-03 masks: 20.6 17.2 [wd: 4.04e-02] [lr: 4.00e-04] [mem: 1.30e+04] (439.4 ms)
[2,   390] grad_stats: [6.78e-04 2.38e-05] (1.35e-05, 7.65e-04)
[2,   780] loss: 3.695239e-03 masks: 20.3 17.3 [wd: 4.06e-02] [lr: 4.67e-04] [mem: 1.30e+04] (437.5 ms)
[2,   780] grad_stats: [5.76e-04 4.38e-05] (1.62e-05, 9.45e-04)
avg. loss 3.69523936e-03
time taken for epoch 0:05:54.995957
Epoch 3
[3,     0] loss: 3.426610e-03 masks: 33.0 12.0 [wd: 4.06e-02] [lr: 4.67e-04] [mem: 1.30e+04] (504.9 ms)
[3,     0] grad_stats: [8.67e-04 6.30e-05] (2.47e-05, 1.37e-03)
[3,   390] loss: 3.228268e-03 masks: 20.8 17.1 [wd: 4.10e-02] [lr: 5.33e-04] [mem: 1.33e+04] (440.8 ms)
[3,   390] grad_stats: [7.98e-04 3.90e-05] (1.56e-05, 1.13e-03)
[3,   780] loss: 3.235173e-03 masks: 20.7 17.2 [wd: 4.14e-02] [lr: 6.00e-04] [mem: 1.33e+04] (440.5 ms)
[3,   780] grad_stats: [8.66e-04 2.83e-05] (1.15e-05, 8.82e-04)
avg. loss 3.23517337e-03
time taken for epoch 0:05:57.345259
Epoch 4
[4,     0] loss: 2.901598e-03 masks: 18.0 20.0 [wd: 4.14e-02] [lr: 6.00e-04] [mem: 1.33e+04] (425.7 ms)
[4,     0] grad_stats: [3.47e-04 1.35e-05] (6.61e-06, 7.97e-04)
[4,   390] loss: 3.276600e-03 masks: 20.2 17.3 [wd: 4.19e-02] [lr: 6.67e-04] [mem: 1.33e+04] (436.9 ms)
[4,   390] grad_stats: [7.45e-04 3.50e-05] (8.52e-06, 1.04e-03)
[4,   780] loss: 3.207296e-03 masks: 20.1 17.3 [wd: 4.25e-02] [lr: 7.33e-04] [mem: 1.33e+04] (436.5 ms)
[4,   780] grad_stats: [4.27e-04 2.76e-05] (5.48e-06, 6.95e-04)
avg. loss 3.20729621e-03
time taken for epoch 0:05:54.380856
Epoch 5
[5,     0] loss: 2.878746e-03 masks: 16.0 20.0 [wd: 4.25e-02] [lr: 7.34e-04] [mem: 1.33e+04] (405.8 ms)
[5,     0] grad_stats: [3.47e-04 2.57e-05] (5.50e-06, 4.31e-04)
[5,   390] loss: 3.063903e-03 masks: 20.4 17.2 [wd: 4.32e-02] [lr: 8.00e-04] [mem: 1.33e+04] (438.3 ms)
[5,   390] grad_stats: [7.59e-04 4.11e-05] (8.79e-06, 8.83e-04)
[5,   780] loss: 3.020571e-03 masks: 20.2 17.3 [wd: 4.39e-02] [lr: 8.67e-04] [mem: 1.33e+04] (437.0 ms)
[5,   780] grad_stats: [2.77e-04 4.31e-05] (6.00e-06, 3.68e-04)
avg. loss 3.02057108e-03
time taken for epoch 0:05:54.738204
Epoch 6
[6,     0] loss: 3.025571e-03 masks: 23.0 16.0 [wd: 4.39e-02] [lr: 8.67e-04] [mem: 1.33e+04] (447.5 ms)
[6,     0] grad_stats: [4.08e-04 2.34e-05] (5.34e-06, 4.08e-04)
[6,   390] loss: 2.981596e-03 masks: 20.4 17.2 [wd: 4.48e-02] [lr: 9.33e-04] [mem: 1.33e+04] (438.4 ms)
[6,   390] grad_stats: [1.59e-04 1.43e-05] (3.25e-06, 2.43e-04)
[6,   780] loss: 2.894446e-03 masks: 20.4 17.2 [wd: 4.57e-02] [lr: 1.00e-03] [mem: 1.33e+04] (438.8 ms)
[6,   780] grad_stats: [2.28e-04 2.24e-05] (4.27e-06, 2.88e-04)
avg. loss 2.89444600e-03
time taken for epoch 0:05:55.974730
Epoch 7
[7,     0] loss: 2.791499e-03 masks: 23.0 16.0 [wd: 4.57e-02] [lr: 1.00e-03] [mem: 1.33e+04] (448.1 ms)
[7,     0] grad_stats: [1.89e-04 2.17e-05] (3.61e-06, 2.60e-04)
[7,   390] loss: 2.818071e-03 masks: 20.1 17.3 [wd: 4.66e-02] [lr: 1.00e-03] [mem: 1.33e+04] (435.8 ms)
[7,   390] grad_stats: [3.22e-04 1.84e-05] (3.10e-06, 3.61e-04)
[7,   780] loss: 2.793763e-03 masks: 20.1 17.2 [wd: 4.77e-02] [lr: 9.99e-04] [mem: 1.33e+04] (436.0 ms)
[7,   780] grad_stats: [3.08e-04 2.97e-05] (4.35e-06, 4.14e-04)
avg. loss 2.79376279e-03
time taken for epoch 0:05:53.753019
Epoch 8
[8,     0] loss: 2.628763e-03 masks: 24.0 16.0 [wd: 4.77e-02] [lr: 9.99e-04] [mem: 1.33e+04] (454.8 ms)
[8,     0] grad_stats: [1.71e-04 2.01e-05] (4.04e-06, 2.61e-04)
[8,   390] loss: 2.719395e-03 masks: 20.2 17.4 [wd: 4.88e-02] [lr: 9.99e-04] [mem: 1.33e+04] (437.5 ms)
[8,   390] grad_stats: [4.18e-04 3.43e-05] (7.01e-06, 6.72e-04)
[8,   780] loss: 2.697562e-03 masks: 19.9 17.5 [wd: 5.00e-02] [lr: 9.98e-04] [mem: 1.33e+04] (435.8 ms)
[8,   780] grad_stats: [2.75e-04 2.61e-05] (4.84e-06, 4.45e-04)
avg. loss 2.69756156e-03
time taken for epoch 0:05:53.498169
Epoch 9
[9,     0] loss: 2.355247e-03 masks: 17.0 20.0 [wd: 5.00e-02] [lr: 9.98e-04] [mem: 1.33e+04] (417.7 ms)
[9,     0] grad_stats: [1.63e-04 1.51e-05] (3.16e-06, 2.35e-04)
[9,   390] loss: 2.651518e-03 masks: 20.3 17.3 [wd: 5.13e-02] [lr: 9.97e-04] [mem: 1.33e+04] (437.8 ms)
[9,   390] grad_stats: [5.33e-04 2.99e-05] (6.77e-06, 5.33e-04)
[9,   780] loss: 2.653440e-03 masks: 20.3 17.2 [wd: 5.26e-02] [lr: 9.95e-04] [mem: 1.33e+04] (437.8 ms)
[9,   780] grad_stats: [1.65e-04 2.10e-05] (3.20e-06, 2.57e-04)
avg. loss 2.65343991e-03
time taken for epoch 0:05:55.092224
Epoch 10
[10,     0] loss: 2.900877e-03 masks: 34.0 12.0 [wd: 5.26e-02] [lr: 9.95e-04] [mem: 1.33e+04] (516.2 ms)
[10,     0] grad_stats: [3.06e-04 3.67e-05] (5.32e-06, 5.25e-04)
[10,   390] loss: 2.601155e-03 masks: 20.5 17.1 [wd: 5.41e-02] [lr: 9.94e-04] [mem: 1.33e+04] (439.2 ms)
[10,   390] grad_stats: [1.41e-04 1.32e-05] (2.72e-06, 2.54e-04)
[10,   780] loss: 2.588952e-03 masks: 20.5 17.2 [wd: 5.56e-02] [lr: 9.92e-04] [mem: 1.33e+04] (439.2 ms)
[10,   780] grad_stats: [1.78e-04 1.60e-05] (2.78e-06, 2.30e-04)
avg. loss 2.58895213e-03
time taken for epoch 0:05:56.117412
Epoch 11
[11,     0] loss: 2.866716e-03 masks: 17.0 16.0 [wd: 5.56e-02] [lr: 9.92e-04] [mem: 1.33e+04] (396.0 ms)
[11,     0] grad_stats: [2.57e-04 1.74e-05] (2.98e-06, 3.30e-04)
[11,   390] loss: 2.597782e-03 masks: 20.2 17.4 [wd: 5.71e-02] [lr: 9.90e-04] [mem: 1.33e+04] (437.7 ms)
[11,   390] grad_stats: [1.64e-04 1.58e-05] (2.88e-06, 1.99e-04)
[11,   780] loss: 2.610048e-03 masks: 20.5 17.2 [wd: 5.88e-02] [lr: 9.87e-04] [mem: 1.33e+04] (440.2 ms)
[11,   780] grad_stats: [2.21e-04 2.13e-05] (3.77e-06, 3.48e-04)
avg. loss 2.61004753e-03
time taken for epoch 0:05:56.927500
Epoch 12
[12,     0] loss: 2.636265e-03 masks: 23.0 16.0 [wd: 5.88e-02] [lr: 9.87e-04] [mem: 1.33e+04] (448.7 ms)
[12,     0] grad_stats: [1.63e-04 1.82e-05] (3.14e-06, 1.98e-04)
[12,   390] loss: 2.624923e-03 masks: 20.3 17.5 [wd: 6.05e-02] [lr: 9.84e-04] [mem: 1.33e+04] (439.8 ms)
[12,   390] grad_stats: [1.36e-04 1.41e-05] (2.21e-06, 1.52e-04)
[12,   780] loss: 2.634919e-03 masks: 20.4 17.4 [wd: 6.23e-02] [lr: 9.81e-04] [mem: 1.33e+04] (440.0 ms)
[12,   780] grad_stats: [3.03e-04 2.08e-05] (3.84e-06, 3.30e-04)
avg. loss 2.63491920e-03
time taken for epoch 0:05:56.940340
Epoch 13
[13,     0] loss: 2.801366e-03 masks: 20.0 16.0 [wd: 6.23e-02] [lr: 9.81e-04] [mem: 1.33e+04] (422.3 ms)
[13,     0] grad_stats: [3.85e-04 2.50e-05] (4.01e-06, 3.85e-04)
[13,   390] loss: 2.742163e-03 masks: 20.0 17.3 [wd: 6.41e-02] [lr: 9.78e-04] [mem: 1.33e+04] (435.6 ms)
[13,   390] grad_stats: [1.75e-04 1.63e-05] (3.00e-06, 2.85e-04)
[13,   780] loss: 2.744360e-03 masks: 20.0 17.3 [wd: 6.60e-02] [lr: 9.75e-04] [mem: 1.33e+04] (436.0 ms)
[13,   780] grad_stats: [1.45e-04 1.34e-05] (1.97e-06, 1.79e-04)
avg. loss 2.74435983e-03
time taken for epoch 0:05:53.599502
Epoch 14
[14,     0] loss: 2.595979e-03 masks: 16.0 20.0 [wd: 6.60e-02] [lr: 9.75e-04] [mem: 1.33e+04] (406.5 ms)
[14,     0] grad_stats: [1.48e-04 1.36e-05] (2.43e-06, 1.72e-04)
[14,   390] loss: 2.755041e-03 masks: 20.6 17.1 [wd: 6.80e-02] [lr: 9.71e-04] [mem: 1.33e+04] (440.3 ms)
[14,   390] grad_stats: [1.20e-04 1.65e-05] (2.95e-06, 2.34e-04)
[14,   780] loss: 2.777740e-03 masks: 20.3 17.2 [wd: 7.01e-02] [lr: 9.67e-04] [mem: 1.33e+04] (438.3 ms)
[14,   780] grad_stats: [1.16e-04 1.11e-05] (1.81e-06, 1.45e-04)
avg. loss 2.77774029e-03
time taken for epoch 0:05:55.367982
Epoch 15
[15,     0] loss: 2.581589e-03 masks: 24.0 16.0 [wd: 7.01e-02] [lr: 9.67e-04] [mem: 1.33e+04] (454.7 ms)
[15,     0] grad_stats: [3.44e-04 2.37e-05] (4.69e-06, 3.53e-04)
[15,   390] loss: 2.827156e-03 masks: 19.8 17.4 [wd: 7.22e-02] [lr: 9.63e-04] [mem: 1.33e+04] (435.4 ms)
[15,   390] grad_stats: [1.72e-04 1.52e-05] (2.56e-06, 2.04e-04)
[15,   780] loss: 2.850682e-03 masks: 20.2 17.3 [wd: 7.44e-02] [lr: 9.59e-04] [mem: 1.33e+04] (438.4 ms)
[15,   780] grad_stats: [1.14e-04 1.80e-05] (3.05e-06, 1.90e-04)
avg. loss 2.85068222e-03
time taken for epoch 0:05:55.625969
Epoch 16
[16,     0] loss: 3.111196e-03 masks: 12.0 20.0 [wd: 7.44e-02] [lr: 9.59e-04] [mem: 1.33e+04] (370.1 ms)
[16,     0] grad_stats: [1.34e-04 1.36e-05] (2.79e-06, 1.37e-04)
[16,   390] loss: 2.939574e-03 masks: 20.0 17.3 [wd: 7.66e-02] [lr: 9.54e-04] [mem: 1.33e+04] (436.1 ms)
[16,   390] grad_stats: [8.34e-05 1.12e-05] (1.96e-06, 1.50e-04)
[16,   780] loss: 2.950369e-03 masks: 20.1 17.4 [wd: 7.89e-02] [lr: 9.49e-04] [mem: 1.33e+04] (437.5 ms)
[16,   780] grad_stats: [1.22e-04 1.78e-05] (2.98e-06, 1.62e-04)
avg. loss 2.95036945e-03
time taken for epoch 0:05:54.796087
Epoch 17
[17,     0] loss: 2.630810e-03 masks: 18.0 20.0 [wd: 7.89e-02] [lr: 9.49e-04] [mem: 1.33e+04] (426.5 ms)
[17,     0] grad_stats: [1.30e-04 1.65e-05] (3.42e-06, 2.10e-04)
[17,   390] loss: 2.991275e-03 masks: 19.9 17.5 [wd: 8.13e-02] [lr: 9.44e-04] [mem: 1.33e+04] (436.2 ms)
[17,   390] grad_stats: [2.45e-04 1.88e-05] (3.63e-06, 2.80e-04)
[17,   780] loss: 3.018958e-03 masks: 20.0 17.4 [wd: 8.37e-02] [lr: 9.39e-04] [mem: 1.33e+04] (436.6 ms)
[17,   780] grad_stats: [1.50e-04 1.69e-05] (3.72e-06, 1.98e-04)
avg. loss 3.01895822e-03
time taken for epoch 0:05:54.223927
Epoch 18
[18,     0] loss: 2.813220e-03 masks: 16.0 20.0 [wd: 8.37e-02] [lr: 9.39e-04] [mem: 1.33e+04] (406.7 ms)
[18,     0] grad_stats: [9.14e-05 1.54e-05] (2.59e-06, 1.47e-04)
[18,   390] loss: 3.120492e-03 masks: 20.6 17.2 [wd: 8.62e-02] [lr: 9.33e-04] [mem: 1.33e+04] (440.3 ms)
[18,   390] grad_stats: [2.48e-04 1.93e-05] (4.15e-06, 2.72e-04)
[18,   780] loss: 3.199051e-03 masks: 20.4 17.2 [wd: 8.88e-02] [lr: 9.27e-04] [mem: 1.33e+04] (439.5 ms)
[18,   780] grad_stats: [1.85e-04 2.01e-05] (2.97e-06, 3.00e-04)
avg. loss 3.19905055e-03
time taken for epoch 0:05:56.533382
Epoch 19
[19,     0] loss: 3.183946e-03 masks: 13.0 20.0 [wd: 8.88e-02] [lr: 9.27e-04] [mem: 1.33e+04] (382.0 ms)
[19,     0] grad_stats: [1.10e-04 1.30e-05] (2.90e-06, 2.10e-04)
[19,   390] loss: 3.398243e-03 masks: 19.7 17.4 [wd: 9.14e-02] [lr: 9.21e-04] [mem: 1.33e+04] (434.6 ms)
[19,   390] grad_stats: [1.35e-04 1.74e-05] (3.94e-06, 1.99e-04)
[19,   780] loss: 3.432562e-03 masks: 20.0 17.3 [wd: 9.41e-02] [lr: 9.15e-04] [mem: 1.33e+04] (436.4 ms)
[19,   780] grad_stats: [2.14e-04 1.38e-05] (3.18e-06, 2.29e-04)
avg. loss 3.43256247e-03
time taken for epoch 0:05:53.853011
Epoch 20
[20,     0] loss: 3.509783e-03 masks: 13.0 20.0 [wd: 9.41e-02] [lr: 9.15e-04] [mem: 1.33e+04] (382.3 ms)
[20,     0] grad_stats: [7.68e-05 1.35e-05] (2.53e-06, 1.34e-04)
[20,   390] loss: 3.531205e-03 masks: 20.5 17.2 [wd: 9.68e-02] [lr: 9.09e-04] [mem: 1.33e+04] (440.6 ms)
[20,   390] grad_stats: [1.23e-04 1.15e-05] (2.73e-06, 1.92e-04)
[20,   780] loss: 3.586607e-03 masks: 20.5 17.2 [wd: 9.96e-02] [lr: 9.02e-04] [mem: 1.33e+04] (440.5 ms)
[20,   780] grad_stats: [2.37e-04 1.74e-05] (3.92e-06, 2.55e-04)
avg. loss 3.58660685e-03
time taken for epoch 0:05:57.174665
Epoch 21
[21,     0] loss: 3.739942e-03 masks: 25.0 16.0 [wd: 9.96e-02] [lr: 9.02e-04] [mem: 1.33e+04] (468.0 ms)
[21,     0] grad_stats: [2.94e-04 2.24e-05] (4.44e-06, 3.95e-04)
[21,   390] loss: 3.725349e-03 masks: 20.4 17.2 [wd: 1.02e-01] [lr: 8.95e-04] [mem: 1.33e+04] (439.4 ms)
[21,   390] grad_stats: [1.37e-04 1.60e-05] (3.09e-06, 2.08e-04)
[21,   780] loss: 3.776874e-03 masks: 20.1 17.4 [wd: 1.05e-01] [lr: 8.88e-04] [mem: 1.33e+04] (437.2 ms)
[21,   780] grad_stats: [1.35e-04 1.56e-05] (3.25e-06, 1.92e-04)
avg. loss 3.77687391e-03
time taken for epoch 0:05:54.525835
Epoch 22
[22,     0] loss: 3.572055e-03 masks: 16.0 20.0 [wd: 1.05e-01] [lr: 8.88e-04] [mem: 1.33e+04] (406.6 ms)
[22,     0] grad_stats: [1.02e-04 1.59e-05] (2.76e-06, 1.76e-04)
[22,   390] loss: 3.955047e-03 masks: 20.1 17.4 [wd: 1.08e-01] [lr: 8.81e-04] [mem: 1.33e+04] (437.4 ms)
[22,   390] grad_stats: [1.33e-04 1.55e-05] (3.11e-06, 1.99e-04)
[22,   780] loss: 4.034891e-03 masks: 20.4 17.2 [wd: 1.11e-01] [lr: 8.73e-04] [mem: 1.33e+04] (439.2 ms)
[22,   780] grad_stats: [1.21e-04 1.74e-05] (4.25e-06, 3.03e-04)
avg. loss 4.03489135e-03
time taken for epoch 0:05:56.214947
Epoch 23
[23,     0] loss: 4.069259e-03 masks: 23.0 16.0 [wd: 1.11e-01] [lr: 8.73e-04] [mem: 1.33e+04] (448.2 ms)
[23,     0] grad_stats: [1.49e-04 1.41e-05] (2.66e-06, 2.87e-04)
[23,   390] loss: 4.172063e-03 masks: 20.4 17.2 [wd: 1.14e-01] [lr: 8.66e-04] [mem: 1.33e+04] (439.3 ms)
[23,   390] grad_stats: [1.01e-04 1.48e-05] (3.24e-06, 1.67e-04)
[23,   780] loss: 4.195419e-03 masks: 20.3 17.3 [wd: 1.17e-01] [lr: 8.58e-04] [mem: 1.33e+04] (439.1 ms)
[23,   780] grad_stats: [2.37e-04 2.27e-05] (5.32e-06, 2.37e-04)
avg. loss 4.19541854e-03
time taken for epoch 0:05:56.097520
Epoch 24
[24,     0] loss: 4.269249e-03 masks: 21.0 16.0 [wd: 1.17e-01] [lr: 8.58e-04] [mem: 1.33e+04] (429.0 ms)
[24,     0] grad_stats: [2.39e-04 2.55e-05] (5.06e-06, 2.39e-04)
[24,   390] loss: 4.296698e-03 masks: 20.3 17.3 [wd: 1.20e-01] [lr: 8.50e-04] [mem: 1.33e+04] (438.3 ms)
[24,   390] grad_stats: [1.62e-04 2.15e-05] (5.22e-06, 4.29e-04)
[24,   780] loss: 4.342992e-03 masks: 20.5 17.2 [wd: 1.24e-01] [lr: 8.41e-04] [mem: 1.33e+04] (440.3 ms)
[24,   780] grad_stats: [1.71e-04 3.52e-05] (9.46e-06, 8.25e-04)
avg. loss 4.34299193e-03
time taken for epoch 0:05:57.017192
Epoch 25
[25,     0] loss: 4.684169e-03 masks: 22.0 16.0 [wd: 1.24e-01] [lr: 8.41e-04] [mem: 1.33e+04] (435.6 ms)
[25,     0] grad_stats: [1.68e-04 1.94e-05] (4.74e-06, 2.49e-04)
[25,   390] loss: 4.508491e-03 masks: 20.3 17.3 [wd: 1.27e-01] [lr: 8.33e-04] [mem: 1.33e+04] (438.9 ms)
[25,   390] grad_stats: [2.07e-04 2.52e-05] (6.22e-06, 4.43e-04)
[25,   780] loss: 4.550222e-03 masks: 20.1 17.4 [wd: 1.30e-01] [lr: 8.24e-04] [mem: 1.33e+04] (437.9 ms)
[25,   780] grad_stats: [3.16e-04 2.09e-05] (5.22e-06, 3.60e-04)
avg. loss 4.55022206e-03
time taken for epoch 0:05:55.139375
Epoch 26
[26,     0] loss: 4.446496e-03 masks: 25.0 16.0 [wd: 1.30e-01] [lr: 8.24e-04] [mem: 1.33e+04] (467.5 ms)
[26,     0] grad_stats: [2.77e-04 1.90e-05] (4.08e-06, 4.09e-04)
[26,   390] loss: 4.691613e-03 masks: 20.3 17.3 [wd: 1.33e-01] [lr: 8.16e-04] [mem: 1.33e+04] (439.1 ms)
[26,   390] grad_stats: [1.80e-04 1.92e-05] (5.35e-06, 2.29e-04)
[26,   780] loss: 4.747754e-03 masks: 20.2 17.3 [wd: 1.37e-01] [lr: 8.07e-04] [mem: 1.33e+04] (438.8 ms)
[26,   780] grad_stats: [5.42e-04 2.71e-05] (7.90e-06, 5.44e-04)
avg. loss 4.74775443e-03
time taken for epoch 0:05:55.824412
Epoch 27
[27,     0] loss: 5.131077e-03 masks: 20.0 16.0 [wd: 1.37e-01] [lr: 8.07e-04] [mem: 1.33e+04] (422.8 ms)
[27,     0] grad_stats: [2.90e-04 2.12e-05] (4.62e-06, 3.47e-04)
[27,   390] loss: 4.902472e-03 masks: 20.2 17.4 [wd: 1.40e-01] [lr: 7.98e-04] [mem: 1.33e+04] (438.4 ms)
[27,   390] grad_stats: [2.84e-04 1.92e-05] (4.90e-06, 3.19e-04)
[27,   780] loss: 4.927843e-03 masks: 20.1 17.4 [wd: 1.43e-01] [lr: 7.89e-04] [mem: 1.33e+04] (437.7 ms)
[27,   780] grad_stats: [7.69e-05 1.56e-05] (4.23e-06, 2.35e-04)
avg. loss 4.92784263e-03
time taken for epoch 0:05:54.998562
Epoch 28
[28,     0] loss: 4.821747e-03 masks: 25.0 16.0 [wd: 1.43e-01] [lr: 7.89e-04] [mem: 1.33e+04] (468.4 ms)
[28,     0] grad_stats: [1.37e-04 1.81e-05] (4.38e-06, 4.38e-04)
[28,   390] loss: 5.103753e-03 masks: 20.5 17.1 [wd: 1.47e-01] [lr: 7.79e-04] [mem: 1.33e+04] (439.5 ms)
[28,   390] grad_stats: [1.17e-04 2.47e-05] (5.86e-06, 4.84e-04)
[28,   780] loss: 5.156144e-03 masks: 20.5 17.1 [wd: 1.50e-01] [lr: 7.70e-04] [mem: 1.33e+04] (439.4 ms)
[28,   780] grad_stats: [1.70e-04 3.36e-05] (7.28e-06, 4.10e-04)
avg. loss 5.15614381e-03
time taken for epoch 0:05:56.305355
Epoch 29
[29,     0] loss: 5.546660e-03 masks: 21.0 16.0 [wd: 1.50e-01] [lr: 7.70e-04] [mem: 1.33e+04] (428.5 ms)
[29,     0] grad_stats: [1.60e-04 2.26e-05] (5.09e-06, 4.51e-04)
[29,   390] loss: 5.326164e-03 masks: 20.5 17.1 [wd: 1.54e-01] [lr: 7.60e-04] [mem: 1.33e+04] (439.6 ms)
[29,   390] grad_stats: [2.24e-04 3.12e-05] (6.38e-06, 6.89e-04)
[29,   780] loss: 5.335029e-03 masks: 20.3 17.2 [wd: 1.57e-01] [lr: 7.50e-04] [mem: 1.33e+04] (438.8 ms)
[29,   780] grad_stats: [7.14e-05 2.65e-05] (5.99e-06, 4.62e-04)
avg. loss 5.33502857e-03
time taken for epoch 0:05:55.809933
Epoch 30
[30,     0] loss: 4.695053e-03 masks: 17.0 20.0 [wd: 1.57e-01] [lr: 7.50e-04] [mem: 1.33e+04] (419.4 ms)
[30,     0] grad_stats: [7.52e-05 2.00e-05] (4.97e-06, 5.05e-04)
[30,   390] loss: 5.452891e-03 masks: 20.3 17.3 [wd: 1.61e-01] [lr: 7.40e-04] [mem: 1.33e+04] (439.0 ms)
[30,   390] grad_stats: [1.04e-04 2.08e-05] (4.69e-06, 4.59e-04)
[30,   780] loss: 5.489958e-03 masks: 20.4 17.2 [wd: 1.64e-01] [lr: 7.30e-04] [mem: 1.33e+04] (439.6 ms)
[30,   780] grad_stats: [1.98e-04 1.91e-05] (4.77e-06, 4.54e-04)
avg. loss 5.48995792e-03
time taken for epoch 0:05:56.497844
Epoch 31
[31,     0] loss: 4.942400e-03 masks: 16.0 20.0 [wd: 1.64e-01] [lr: 7.30e-04] [mem: 1.33e+04] (406.7 ms)
[31,     0] grad_stats: [1.62e-04 1.61e-05] (4.20e-06, 5.13e-04)
[31,   390] loss: 5.574273e-03 masks: 20.3 17.4 [wd: 1.68e-01] [lr: 7.20e-04] [mem: 1.33e+04] (439.6 ms)
[31,   390] grad_stats: [1.09e-04 1.85e-05] (4.94e-06, 4.35e-04)
[31,   780] loss: 5.651381e-03 masks: 20.5 17.3 [wd: 1.72e-01] [lr: 7.10e-04] [mem: 1.33e+04] (440.1 ms)
[31,   780] grad_stats: [7.61e-05 1.93e-05] (5.05e-06, 5.56e-04)
avg. loss 5.65138125e-03
time taken for epoch 0:05:56.865055
Epoch 32
[32,     0] loss: 5.579394e-03 masks: 24.0 16.0 [wd: 1.72e-01] [lr: 7.10e-04] [mem: 1.33e+04] (454.1 ms)
[32,     0] grad_stats: [1.00e-04 2.99e-05] (8.12e-06, 4.19e-04)
[32,   390] loss: 5.738948e-03 masks: 20.1 17.4 [wd: 1.75e-01] [lr: 6.99e-04] [mem: 1.33e+04] (437.2 ms)
[32,   390] grad_stats: [1.37e-04 2.79e-05] (7.26e-06, 5.74e-04)
[32,   780] loss: 5.775463e-03 masks: 20.0 17.4 [wd: 1.79e-01] [lr: 6.89e-04] [mem: 1.33e+04] (436.8 ms)
[32,   780] grad_stats: [1.03e-04 2.49e-05] (5.61e-06, 3.93e-04)
avg. loss 5.77546281e-03
time taken for epoch 0:05:54.329088
Epoch 33
[33,     0] loss: 5.569286e-03 masks: 23.0 16.0 [wd: 1.79e-01] [lr: 6.89e-04] [mem: 1.33e+04] (448.8 ms)
[33,     0] grad_stats: [2.29e-04 2.36e-05] (6.28e-06, 6.37e-04)
[33,   390] loss: 5.874129e-03 masks: 20.6 17.2 [wd: 1.83e-01] [lr: 6.78e-04] [mem: 1.33e+04] (440.7 ms)
[33,   390] grad_stats: [1.20e-04 2.08e-05] (5.29e-06, 2.58e-04)
[33,   780] loss: 5.915120e-03 masks: 20.5 17.2 [wd: 1.86e-01] [lr: 6.68e-04] [mem: 1.33e+04] (440.1 ms)
[33,   780] grad_stats: [2.02e-04 2.46e-05] (8.35e-06, 6.82e-04)
avg. loss 5.91511988e-03
time taken for epoch 0:05:56.814667
Epoch 34
[34,     0] loss: 4.961886e-03 masks: 18.0 20.0 [wd: 1.86e-01] [lr: 6.68e-04] [mem: 1.33e+04] (427.0 ms)
[34,     0] grad_stats: [7.78e-05 1.75e-05] (4.82e-06, 3.74e-04)
[34,   390] loss: 6.039271e-03 masks: 20.2 17.3 [wd: 1.90e-01] [lr: 6.57e-04] [mem: 1.33e+04] (437.7 ms)
[34,   390] grad_stats: [2.19e-04 3.13e-05] (8.11e-06, 7.11e-04)
[34,   780] loss: 6.071219e-03 masks: 20.1 17.3 [wd: 1.94e-01] [lr: 6.46e-04] [mem: 1.33e+04] (437.1 ms)
[34,   780] grad_stats: [1.47e-04 2.57e-05] (5.73e-06, 5.21e-04)
avg. loss 6.07121899e-03
time taken for epoch 0:05:54.408167
Epoch 35
[35,     0] loss: 6.577864e-03 masks: 21.0 16.0 [wd: 1.94e-01] [lr: 6.46e-04] [mem: 1.33e+04] (429.6 ms)
[35,     0] grad_stats: [1.11e-04 2.67e-05] (6.76e-06, 4.22e-04)
[35,   390] loss: 6.134826e-03 masks: 20.1 17.3 [wd: 1.97e-01] [lr: 6.35e-04] [mem: 1.33e+04] (437.7 ms)
[35,   390] grad_stats: [1.10e-04 3.24e-05] (8.70e-06, 1.14e-03)
[35,   780] loss: 6.135418e-03 masks: 20.4 17.3 [wd: 2.01e-01] [lr: 6.24e-04] [mem: 1.33e+04] (439.7 ms)
[35,   780] grad_stats: [6.18e-05 2.69e-05] (8.22e-06, 4.04e-04)
avg. loss 6.13541792e-03
time taken for epoch 0:05:56.557751
Epoch 36
[36,     0] loss: 5.264645e-03 masks: 19.0 20.0 [wd: 2.01e-01] [lr: 6.24e-04] [mem: 1.33e+04] (438.7 ms)
[36,     0] grad_stats: [8.37e-05 2.99e-05] (7.46e-06, 5.24e-04)
[36,   390] loss: 6.205888e-03 masks: 20.0 17.4 [wd: 2.05e-01] [lr: 6.13e-04] [mem: 1.33e+04] (437.2 ms)
[36,   390] grad_stats: [1.01e-04 2.83e-05] (7.26e-06, 4.04e-04)
[36,   780] loss: 6.289645e-03 masks: 20.5 17.2 [wd: 2.09e-01] [lr: 6.02e-04] [mem: 1.33e+04] (440.1 ms)
[36,   780] grad_stats: [7.50e-05 2.18e-05] (6.45e-06, 3.64e-04)
avg. loss 6.28964469e-03
time taken for epoch 0:05:56.864739
Epoch 37
[37,     0] loss: 6.595156e-03 masks: 20.0 16.0 [wd: 2.09e-01] [lr: 6.02e-04] [mem: 1.33e+04] (423.0 ms)
[37,     0] grad_stats: [2.01e-04 3.55e-05] (8.02e-06, 6.59e-04)
[37,   390] loss: 6.367797e-03 masks: 20.2 17.3 [wd: 2.12e-01] [lr: 5.91e-04] [mem: 1.33e+04] (438.1 ms)
[37,   390] grad_stats: [8.45e-05 2.32e-05] (5.43e-06, 8.40e-04)
[37,   780] loss: 6.375293e-03 masks: 20.2 17.3 [wd: 2.16e-01] [lr: 5.80e-04] [mem: 1.33e+04] (438.6 ms)
[37,   780] grad_stats: [5.62e-05 2.27e-05] (6.78e-06, 4.09e-04)
avg. loss 6.37529253e-03
time taken for epoch 0:05:55.745035
Epoch 38
[38,     0] loss: 6.855581e-03 masks: 23.0 16.0 [wd: 2.16e-01] [lr: 5.80e-04] [mem: 1.33e+04] (450.0 ms)
[38,     0] grad_stats: [7.52e-05 2.78e-05] (9.28e-06, 7.11e-04)
[38,   390] loss: 6.436254e-03 masks: 20.2 17.4 [wd: 2.20e-01] [lr: 5.69e-04] [mem: 1.33e+04] (438.5 ms)
[38,   390] grad_stats: [2.59e-04 2.73e-05] (8.17e-06, 7.93e-04)
[38,   780] loss: 6.445945e-03 masks: 20.3 17.3 [wd: 2.24e-01] [lr: 5.57e-04] [mem: 1.33e+04] (439.1 ms)
[38,   780] grad_stats: [5.35e-05 2.51e-05] (6.15e-06, 4.58e-04)
avg. loss 6.44594522e-03
time taken for epoch 0:05:56.054403
Epoch 39
[39,     0] loss: 6.235248e-03 masks: 26.0 16.0 [wd: 2.24e-01] [lr: 5.57e-04] [mem: 1.33e+04] (479.4 ms)
[39,     0] grad_stats: [1.00e-04 2.62e-05] (8.26e-06, 1.04e-03)
[39,   390] loss: 6.454763e-03 masks: 20.2 17.4 [wd: 2.28e-01] [lr: 5.46e-04] [mem: 1.33e+04] (438.9 ms)
[39,   390] grad_stats: [1.73e-04 3.45e-05] (8.46e-06, 1.15e-03)
[39,   780] loss: 6.515611e-03 masks: 20.3 17.3 [wd: 2.31e-01] [lr: 5.35e-04] [mem: 1.33e+04] (439.3 ms)
[39,   780] grad_stats: [6.11e-05 2.80e-05] (8.16e-06, 6.08e-04)
avg. loss 6.51561061e-03
time taken for epoch 0:05:56.154211
Epoch 40
[40,     0] loss: 5.872798e-03 masks: 16.0 20.0 [wd: 2.31e-01] [lr: 5.35e-04] [mem: 1.33e+04] (407.3 ms)
[40,     0] grad_stats: [1.29e-04 2.69e-05] (8.39e-06, 8.13e-04)
[40,   390] loss: 6.540439e-03 masks: 19.6 17.6 [wd: 2.35e-01] [lr: 5.23e-04] [mem: 1.33e+04] (435.0 ms)
[40,   390] grad_stats: [1.49e-04 2.43e-05] (9.16e-06, 4.71e-04)
[40,   780] loss: 6.597392e-03 masks: 20.0 17.4 [wd: 2.39e-01] [lr: 5.12e-04] [mem: 1.33e+04] (436.7 ms)
[40,   780] grad_stats: [1.17e-04 3.32e-05] (1.07e-05, 1.23e-03)
avg. loss 6.59739207e-03
time taken for epoch 0:05:54.187682
Epoch 41
[41,     0] loss: 7.170593e-03 masks: 20.0 16.0 [wd: 2.39e-01] [lr: 5.12e-04] [mem: 1.33e+04] (422.9 ms)
[41,     0] grad_stats: [1.49e-04 3.02e-05] (1.00e-05, 1.65e-03)
[41,   390] loss: 6.628135e-03 masks: 20.4 17.3 [wd: 2.43e-01] [lr: 5.00e-04] [mem: 1.33e+04] (439.9 ms)
[41,   390] grad_stats: [2.18e-04 3.75e-05] (1.40e-05, 1.08e-03)
[41,   780] loss: 6.664271e-03 masks: 20.4 17.3 [wd: 2.46e-01] [lr: 4.89e-04] [mem: 1.33e+04] (439.9 ms)
[41,   780] grad_stats: [1.12e-04 2.61e-05] (9.82e-06, 4.77e-04)
avg. loss 6.66427064e-03
time taken for epoch 0:05:56.707339
Epoch 42
[42,     0] loss: 6.720150e-03 masks: 24.0 16.0 [wd: 2.46e-01] [lr: 4.89e-04] [mem: 1.33e+04] (455.0 ms)
[42,     0] grad_stats: [1.20e-04 2.92e-05] (1.04e-05, 8.54e-04)
[42,   390] loss: 6.676374e-03 masks: 20.0 17.4 [wd: 2.50e-01] [lr: 4.78e-04] [mem: 1.33e+04] (437.6 ms)
[42,   390] grad_stats: [1.12e-04 3.06e-05] (9.30e-06, 8.16e-04)
[42,   780] loss: 6.709407e-03 masks: 20.3 17.3 [wd: 2.54e-01] [lr: 4.66e-04] [mem: 1.33e+04] (439.2 ms)
[42,   780] grad_stats: [1.27e-04 4.35e-05] (1.45e-05, 1.51e-03)
avg. loss 6.70940683e-03
time taken for epoch 0:05:56.166229
Epoch 43
[43,     0] loss: 5.864124e-03 masks: 18.0 20.0 [wd: 2.54e-01] [lr: 4.66e-04] [mem: 1.33e+04] (426.8 ms)
[43,     0] grad_stats: [1.59e-04 3.18e-05] (9.58e-06, 9.19e-04)
[43,   390] loss: 6.778174e-03 masks: 20.1 17.3 [wd: 2.57e-01] [lr: 4.55e-04] [mem: 1.33e+04] (437.4 ms)
[43,   390] grad_stats: [8.77e-05 4.05e-05] (1.14e-05, 1.17e-03)
[43,   780] loss: 6.793733e-03 masks: 20.2 17.3 [wd: 2.61e-01] [lr: 4.44e-04] [mem: 1.33e+04] (438.5 ms)
[43,   780] grad_stats: [1.81e-04 2.89e-05] (9.36e-06, 7.40e-04)
avg. loss 6.79373338e-03
time taken for epoch 0:05:55.698166
Epoch 44
[44,     0] loss: 6.586587e-03 masks: 24.0 16.0 [wd: 2.61e-01] [lr: 4.44e-04] [mem: 1.33e+04] (454.9 ms)
[44,     0] grad_stats: [1.61e-04 3.56e-05] (9.73e-06, 6.02e-04)
[44,   390] loss: 6.812377e-03 masks: 20.3 17.2 [wd: 2.65e-01] [lr: 4.32e-04] [mem: 1.33e+04] (438.2 ms)
[44,   390] grad_stats: [1.22e-04 4.76e-05] (1.13e-05, 1.07e-03)
[44,   780] loss: 6.823457e-03 masks: 20.5 17.2 [wd: 2.68e-01] [lr: 4.21e-04] [mem: 1.33e+04] (440.0 ms)
[44,   780] grad_stats: [1.32e-04 3.71e-05] (1.03e-05, 9.01e-04)
avg. loss 6.82345743e-03
time taken for epoch 0:05:56.993169
Epoch 45
[45,     0] loss: 6.005944e-03 masks: 17.0 20.0 [wd: 2.68e-01] [lr: 4.21e-04] [mem: 1.33e+04] (419.0 ms)
[45,     0] grad_stats: [2.01e-04 3.40e-05] (9.24e-06, 1.18e-03)
[45,   390] loss: 6.778409e-03 masks: 20.3 17.4 [wd: 2.72e-01] [lr: 4.10e-04] [mem: 1.33e+04] (439.1 ms)
[45,   390] grad_stats: [2.21e-04 4.14e-05] (1.22e-05, 1.70e-03)
[45,   780] loss: 6.828463e-03 masks: 20.4 17.2 [wd: 2.76e-01] [lr: 3.99e-04] [mem: 1.33e+04] (439.7 ms)
[45,   780] grad_stats: [1.08e-04 3.31e-05] (1.25e-05, 6.13e-04)
avg. loss 6.82846269e-03
time taken for epoch 0:05:56.572573
Epoch 46
[46,     0] loss: 7.400760e-03 masks: 20.0 16.0 [wd: 2.76e-01] [lr: 3.99e-04] [mem: 1.33e+04] (423.2 ms)
[46,     0] grad_stats: [1.24e-04 3.81e-05] (1.34e-05, 8.17e-04)
[46,   390] loss: 6.802741e-03 masks: 20.7 17.2 [wd: 2.79e-01] [lr: 3.88e-04] [mem: 1.33e+04] (442.6 ms)
[46,   390] grad_stats: [8.02e-05 2.47e-05] (1.10e-05, 5.64e-04)
[46,   780] loss: 6.829286e-03 masks: 20.6 17.2 [wd: 2.83e-01] [lr: 3.77e-04] [mem: 1.33e+04] (441.9 ms)
[46,   780] grad_stats: [1.19e-04 3.43e-05] (1.28e-05, 6.44e-04)
avg. loss 6.82928604e-03
time taken for epoch 0:05:58.229807
Epoch 47
[47,     0] loss: 6.802971e-03 masks: 12.0 20.0 [wd: 2.83e-01] [lr: 3.77e-04] [mem: 1.33e+04] (370.3 ms)
[47,     0] grad_stats: [6.91e-05 3.01e-05] (1.01e-05, 1.05e-03)
[47,   390] loss: 6.880631e-03 masks: 20.6 17.1 [wd: 2.86e-01] [lr: 3.66e-04] [mem: 1.33e+04] (440.6 ms)
[47,   390] grad_stats: [1.90e-04 4.13e-05] (1.55e-05, 1.03e-03)
[47,   780] loss: 6.877954e-03 masks: 20.5 17.2 [wd: 2.90e-01] [lr: 3.55e-04] [mem: 1.33e+04] (440.4 ms)
[47,   780] grad_stats: [1.80e-04 4.92e-05] (1.59e-05, 1.26e-03)
avg. loss 6.87795354e-03
time taken for epoch 0:05:57.035182
Epoch 48
[48,     0] loss: 5.938252e-03 masks: 16.0 20.0 [wd: 2.90e-01] [lr: 3.55e-04] [mem: 1.33e+04] (406.6 ms)
[48,     0] grad_stats: [1.70e-04 3.22e-05] (1.23e-05, 8.92e-04)
[48,   390] loss: 6.890317e-03 masks: 20.2 17.3 [wd: 2.93e-01] [lr: 3.44e-04] [mem: 1.33e+04] (438.5 ms)
[48,   390] grad_stats: [7.95e-05 4.21e-05] (1.48e-05, 1.01e-03)
[48,   780] loss: 6.879950e-03 masks: 20.4 17.3 [wd: 2.97e-01] [lr: 3.33e-04] [mem: 1.33e+04] (440.4 ms)
[48,   780] grad_stats: [1.34e-04 3.77e-05] (1.37e-05, 8.85e-04)
avg. loss 6.87994988e-03
time taken for epoch 0:05:57.141612
Epoch 49
[49,     0] loss: 6.476476e-03 masks: 13.0 20.0 [wd: 2.97e-01] [lr: 3.33e-04] [mem: 1.33e+04] (383.0 ms)
[49,     0] grad_stats: [8.99e-05 3.06e-05] (1.10e-05, 7.17e-04)
[49,   390] loss: 6.892226e-03 masks: 19.8 17.4 [wd: 3.00e-01] [lr: 3.23e-04] [mem: 1.33e+04] (435.8 ms)
[49,   390] grad_stats: [1.19e-04 4.27e-05] (1.42e-05, 1.05e-03)
[49,   780] loss: 6.888186e-03 masks: 20.0 17.4 [wd: 3.03e-01] [lr: 3.12e-04] [mem: 1.33e+04] (437.2 ms)
[49,   780] grad_stats: [8.96e-05 3.82e-05] (1.50e-05, 1.23e-03)
avg. loss 6.88818636e-03
time taken for epoch 0:05:54.603943
Epoch 50
[50,     0] loss: 5.852048e-03 masks: 19.0 20.0 [wd: 3.03e-01] [lr: 3.12e-04] [mem: 1.33e+04] (438.8 ms)
[50,     0] grad_stats: [9.63e-05 2.81e-05] (1.03e-05, 6.83e-04)
[50,   390] loss: 6.920739e-03 masks: 20.3 17.2 [wd: 3.07e-01] [lr: 3.01e-04] [mem: 1.33e+04] (439.5 ms)
[50,   390] grad_stats: [1.61e-04 4.07e-05] (1.70e-05, 1.86e-03)
[50,   780] loss: 6.915140e-03 masks: 20.2 17.3 [wd: 3.10e-01] [lr: 2.91e-04] [mem: 1.33e+04] (438.7 ms)
[50,   780] grad_stats: [8.34e-05 3.84e-05] (1.61e-05, 1.28e-03)
avg. loss 6.91513951e-03
time taken for epoch 0:05:55.763012
Epoch 51
[51,     0] loss: 6.028988e-03 masks: 18.0 20.0 [wd: 3.10e-01] [lr: 2.91e-04] [mem: 1.33e+04] (427.8 ms)
[51,     0] grad_stats: [1.33e-04 3.63e-05] (1.30e-05, 9.42e-04)
[51,   390] loss: 6.937391e-03 masks: 20.5 17.2 [wd: 3.13e-01] [lr: 2.81e-04] [mem: 1.33e+04] (439.8 ms)
[51,   390] grad_stats: [7.20e-05 3.50e-05] (1.36e-05, 1.01e-03)
[51,   780] loss: 6.940421e-03 masks: 20.5 17.2 [wd: 3.16e-01] [lr: 2.71e-04] [mem: 1.33e+04] (440.6 ms)
[51,   780] grad_stats: [1.46e-04 4.09e-05] (1.60e-05, 9.64e-04)
avg. loss 6.94042129e-03
time taken for epoch 0:05:57.301089
Epoch 52
[52,     0] loss: 7.209380e-03 masks: 23.0 16.0 [wd: 3.16e-01] [lr: 2.71e-04] [mem: 1.33e+04] (448.6 ms)
[52,     0] grad_stats: [1.23e-04 3.97e-05] (1.84e-05, 9.99e-04)
[52,   390] loss: 6.907839e-03 masks: 20.1 17.4 [wd: 3.20e-01] [lr: 2.61e-04] [mem: 1.33e+04] (438.4 ms)
[52,   390] grad_stats: [9.85e-05 4.02e-05] (1.54e-05, 7.58e-04)
[52,   780] loss: 6.921871e-03 masks: 20.2 17.4 [wd: 3.23e-01] [lr: 2.51e-04] [mem: 1.33e+04] (438.6 ms)
[52,   780] grad_stats: [1.66e-04 4.24e-05] (1.81e-05, 1.21e-03)
avg. loss 6.92187142e-03
time taken for epoch 0:05:55.772430
Epoch 53
[53,     0] loss: 7.383507e-03 masks: 23.0 16.0 [wd: 3.23e-01] [lr: 2.51e-04] [mem: 1.33e+04] (449.0 ms)
[53,     0] grad_stats: [2.93e-04 5.08e-05] (2.03e-05, 1.67e-03)
[53,   390] loss: 6.932129e-03 masks: 20.4 17.2 [wd: 3.26e-01] [lr: 2.41e-04] [mem: 1.33e+04] (439.7 ms)
[53,   390] grad_stats: [1.48e-04 4.78e-05] (1.87e-05, 9.92e-04)
[53,   780] loss: 6.928405e-03 masks: 20.3 17.3 [wd: 3.29e-01] [lr: 2.31e-04] [mem: 1.33e+04] (439.2 ms)
[53,   780] grad_stats: [9.96e-05 3.87e-05] (1.61e-05, 2.04e-03)
avg. loss 6.92840532e-03
time taken for epoch 0:05:56.178105
Epoch 54
[54,     0] loss: 7.079355e-03 masks: 25.0 16.0 [wd: 3.29e-01] [lr: 2.31e-04] [mem: 1.33e+04] (469.0 ms)
[54,     0] grad_stats: [1.88e-04 4.76e-05] (1.84e-05, 1.26e-03)
[54,   390] loss: 6.882842e-03 masks: 19.8 17.6 [wd: 3.32e-01] [lr: 2.22e-04] [mem: 1.33e+04] (436.1 ms)
[54,   390] grad_stats: [9.61e-05 6.23e-05] (1.91e-05, 8.24e-04)
[54,   780] loss: 6.912298e-03 masks: 19.9 17.4 [wd: 3.35e-01] [lr: 2.12e-04] [mem: 1.33e+04] (436.6 ms)
[54,   780] grad_stats: [7.34e-05 4.08e-05] (1.71e-05, 8.20e-04)
avg. loss 6.91229849e-03
time taken for epoch 0:05:54.040292
Epoch 55
[55,     0] loss: 6.518629e-03 masks: 23.0 16.0 [wd: 3.35e-01] [lr: 2.12e-04] [mem: 1.33e+04] (449.0 ms)
[55,     0] grad_stats: [9.95e-05 4.77e-05] (2.19e-05, 2.15e-03)
[55,   390] loss: 6.924838e-03 masks: 19.6 17.5 [wd: 3.38e-01] [lr: 2.03e-04] [mem: 1.33e+04] (433.9 ms)
[55,   390] grad_stats: [1.34e-04 4.65e-05] (1.77e-05, 1.01e-03)
[55,   780] loss: 6.926130e-03 masks: 20.0 17.4 [wd: 3.40e-01] [lr: 1.94e-04] [mem: 1.33e+04] (437.1 ms)
[55,   780] grad_stats: [1.17e-04 4.47e-05] (2.06e-05, 1.18e-03)
avg. loss 6.92612977e-03
time taken for epoch 0:05:54.547381
Epoch 56
[56,     0] loss: 6.955084e-03 masks: 25.0 16.0 [wd: 3.40e-01] [lr: 1.94e-04] [mem: 1.33e+04] (468.6 ms)
[56,     0] grad_stats: [1.76e-04 5.50e-05] (2.02e-05, 1.25e-03)
[56,   390] loss: 7.000030e-03 masks: 21.3 16.9 [wd: 3.43e-01] [lr: 1.85e-04] [mem: 1.33e+04] (445.6 ms)
[56,   390] grad_stats: [8.76e-05 4.27e-05] (1.73e-05, 1.34e-03)
[56,   780] loss: 6.970281e-03 masks: 20.8 17.1 [wd: 3.46e-01] [lr: 1.77e-04] [mem: 1.33e+04] (442.6 ms)
[56,   780] grad_stats: [7.85e-05 4.24e-05] (1.61e-05, 1.19e-03)
avg. loss 6.97028148e-03
time taken for epoch 0:05:58.716627
Epoch 57
[57,     0] loss: 7.422187e-03 masks: 21.0 16.0 [wd: 3.46e-01] [lr: 1.77e-04] [mem: 1.33e+04] (429.4 ms)
[57,     0] grad_stats: [8.51e-05 4.41e-05] (1.97e-05, 1.45e-03)
[57,   390] loss: 6.993375e-03 masks: 20.1 17.2 [wd: 3.49e-01] [lr: 1.68e-04] [mem: 1.33e+04] (436.8 ms)
[57,   390] grad_stats: [9.07e-05 4.09e-05] (1.77e-05, 9.49e-04)
[57,   780] loss: 6.959516e-03 masks: 20.1 17.3 [wd: 3.51e-01] [lr: 1.60e-04] [mem: 1.33e+04] (437.6 ms)
[57,   780] grad_stats: [9.02e-05 4.39e-05] (1.98e-05, 9.54e-04)
avg. loss 6.95951562e-03
time taken for epoch 0:05:54.945502
Epoch 58
[58,     0] loss: 6.995662e-03 masks: 23.0 16.0 [wd: 3.51e-01] [lr: 1.60e-04] [mem: 1.33e+04] (449.0 ms)
[58,     0] grad_stats: [1.17e-04 4.12e-05] (2.09e-05, 1.55e-03)
[58,   390] loss: 6.937981e-03 masks: 20.4 17.3 [wd: 3.54e-01] [lr: 1.51e-04] [mem: 1.33e+04] (440.3 ms)
[58,   390] grad_stats: [1.03e-04 5.25e-05] (2.24e-05, 2.19e-03)
[58,   780] loss: 6.942635e-03 masks: 20.5 17.2 [wd: 3.56e-01] [lr: 1.43e-04] [mem: 1.33e+04] (440.8 ms)
[58,   780] grad_stats: [8.99e-05 6.06e-05] (2.30e-05, 9.04e-04)
avg. loss 6.94263488e-03
time taken for epoch 0:05:57.447275
Epoch 59
[59,     0] loss: 5.966816e-03 masks: 19.0 20.0 [wd: 3.56e-01] [lr: 1.43e-04] [mem: 1.33e+04] (439.4 ms)
[59,     0] grad_stats: [1.50e-04 4.99e-05] (1.89e-05, 1.02e-03)
[59,   390] loss: 6.869555e-03 masks: 20.1 17.5 [wd: 3.59e-01] [lr: 1.35e-04] [mem: 1.33e+04] (438.5 ms)
[59,   390] grad_stats: [1.17e-04 5.70e-05] (2.41e-05, 1.55e-03)
[59,   780] loss: 6.897154e-03 masks: 20.3 17.4 [wd: 3.61e-01] [lr: 1.28e-04] [mem: 1.33e+04] (439.6 ms)
[59,   780] grad_stats: [1.20e-04 5.40e-05] (2.39e-05, 1.47e-03)
avg. loss 6.89715414e-03
time taken for epoch 0:05:56.500835
Epoch 60
[60,     0] loss: 7.163028e-03 masks: 23.0 16.0 [wd: 3.61e-01] [lr: 1.28e-04] [mem: 1.33e+04] (449.2 ms)
[60,     0] grad_stats: [8.84e-05 3.84e-05] (1.90e-05, 7.87e-04)
[60,   390] loss: 6.910384e-03 masks: 19.9 17.4 [wd: 3.63e-01] [lr: 1.20e-04] [mem: 1.33e+04] (436.0 ms)
[60,   390] grad_stats: [1.05e-04 5.11e-05] (2.45e-05, 9.95e-04)
[60,   780] loss: 6.904751e-03 masks: 20.1 17.3 [wd: 3.66e-01] [lr: 1.13e-04] [mem: 1.33e+04] (438.2 ms)
[60,   780] grad_stats: [8.37e-05 4.83e-05] (1.85e-05, 9.57e-04)
avg. loss 6.90475072e-03
time taken for epoch 0:05:55.370153
Epoch 61
[61,     0] loss: 6.861422e-03 masks: 12.0 20.0 [wd: 3.66e-01] [lr: 1.13e-04] [mem: 1.33e+04] (369.3 ms)
[61,     0] grad_stats: [1.10e-04 5.54e-05] (2.14e-05, 9.76e-04)
[61,   390] loss: 6.920607e-03 masks: 20.4 17.2 [wd: 3.68e-01] [lr: 1.06e-04] [mem: 1.33e+04] (439.4 ms)
[61,   390] grad_stats: [1.39e-04 5.73e-05] (2.25e-05, 1.14e-03)
[61,   780] loss: 6.917194e-03 masks: 20.3 17.3 [wd: 3.70e-01] [lr: 9.91e-05] [mem: 1.33e+04] (438.8 ms)
[61,   780] grad_stats: [9.90e-05 4.81e-05] (2.10e-05, 1.23e-03)
avg. loss 6.91719429e-03
time taken for epoch 0:05:55.810295
Epoch 62
[62,     0] loss: 7.723740e-03 masks: 21.0 16.0 [wd: 3.70e-01] [lr: 9.91e-05] [mem: 1.33e+04] (429.2 ms)
[62,     0] grad_stats: [8.02e-05 4.75e-05] (2.12e-05, 1.77e-03)
[62,   390] loss: 6.944029e-03 masks: 20.4 17.2 [wd: 3.72e-01] [lr: 9.24e-05] [mem: 1.33e+04] (438.9 ms)
[62,   390] grad_stats: [1.17e-04 5.29e-05] (2.35e-05, 1.48e-03)
[62,   780] loss: 6.927005e-03 masks: 20.3 17.2 [wd: 3.74e-01] [lr: 8.60e-05] [mem: 1.33e+04] (438.9 ms)
[62,   780] grad_stats: [8.54e-05 4.19e-05] (1.84e-05, 1.08e-03)
avg. loss 6.92700505e-03
time taken for epoch 0:05:55.902979
Epoch 63
[63,     0] loss: 7.274312e-03 masks: 21.0 16.0 [wd: 3.74e-01] [lr: 8.60e-05] [mem: 1.33e+04] (428.6 ms)
[63,     0] grad_stats: [1.25e-04 6.13e-05] (2.29e-05, 1.72e-03)
[63,   390] loss: 6.907167e-03 masks: 20.1 17.3 [wd: 3.76e-01] [lr: 7.97e-05] [mem: 1.33e+04] (437.7 ms)
[63,   390] grad_stats: [6.69e-05 3.27e-05] (1.89e-05, 1.89e-03)
[63,   780] loss: 6.915344e-03 masks: 20.3 17.2 [wd: 3.78e-01] [lr: 7.37e-05] [mem: 1.33e+04] (438.8 ms)
[63,   780] grad_stats: [8.22e-05 6.05e-05] (2.06e-05, 1.46e-03)
avg. loss 6.91534360e-03
time taken for epoch 0:05:55.788831
Epoch 64
[64,     0] loss: 6.777460e-03 masks: 21.0 16.0 [wd: 3.78e-01] [lr: 7.37e-05] [mem: 1.33e+04] (428.8 ms)
[64,     0] grad_stats: [8.72e-05 5.53e-05] (2.49e-05, 1.15e-03)
[64,   390] loss: 6.920370e-03 masks: 20.4 17.1 [wd: 3.80e-01] [lr: 6.79e-05] [mem: 1.33e+04] (439.2 ms)
[64,   390] grad_stats: [6.56e-05 4.51e-05] (2.04e-05, 8.21e-04)
[64,   780] loss: 6.896303e-03 masks: 20.3 17.2 [wd: 3.81e-01] [lr: 6.23e-05] [mem: 1.33e+04] (438.4 ms)
[64,   780] grad_stats: [1.15e-04 5.46e-05] (2.37e-05, 1.72e-03)
avg. loss 6.89630337e-03
time taken for epoch 0:05:55.655844
Epoch 65
[65,     0] loss: 6.761894e-03 masks: 21.0 16.0 [wd: 3.81e-01] [lr: 6.23e-05] [mem: 1.33e+04] (429.3 ms)
[65,     0] grad_stats: [9.78e-05 6.18e-05] (2.31e-05, 1.27e-03)
[65,   390] loss: 6.795208e-03 masks: 20.0 17.5 [wd: 3.83e-01] [lr: 5.70e-05] [mem: 1.33e+04] (437.1 ms)
[65,   390] grad_stats: [1.23e-04 5.95e-05] (3.13e-05, 1.71e-03)
[65,   780] loss: 6.787033e-03 masks: 20.2 17.4 [wd: 3.84e-01] [lr: 5.19e-05] [mem: 1.33e+04] (438.7 ms)
[65,   780] grad_stats: [1.42e-04 5.88e-05] (2.41e-05, 1.11e-03)
avg. loss 6.78703306e-03
time taken for epoch 0:05:55.763507
Epoch 66
[66,     0] loss: 6.995044e-03 masks: 23.0 16.0 [wd: 3.84e-01] [lr: 5.19e-05] [mem: 1.33e+04] (448.8 ms)
[66,     0] grad_stats: [7.14e-05 4.73e-05] (2.06e-05, 1.37e-03)
[66,   390] loss: 6.858976e-03 masks: 20.6 17.1 [wd: 3.86e-01] [lr: 4.70e-05] [mem: 1.33e+04] (441.4 ms)
[66,   390] grad_stats: [6.55e-05 5.30e-05] (2.43e-05, 9.53e-04)
[66,   780] loss: 6.800580e-03 masks: 20.2 17.3 [wd: 3.87e-01] [lr: 4.24e-05] [mem: 1.33e+04] (438.7 ms)
[66,   780] grad_stats: [1.78e-04 5.06e-05] (2.26e-05, 1.48e-03)
avg. loss 6.80058035e-03
time taken for epoch 0:05:55.708364
Epoch 67
[67,     0] loss: 8.682216e-03 masks: 33.0 12.0 [wd: 3.87e-01] [lr: 4.23e-05] [mem: 1.33e+04] (506.3 ms)
[67,     0] grad_stats: [1.62e-04 6.27e-05] (2.76e-05, 1.85e-03)
[67,   390] loss: 6.811480e-03 masks: 20.4 17.2 [wd: 3.89e-01] [lr: 3.79e-05] [mem: 1.33e+04] (439.6 ms)
[67,   390] grad_stats: [9.28e-05 5.74e-05] (2.65e-05, 1.37e-03)
[67,   780] loss: 6.790328e-03 masks: 20.3 17.3 [wd: 3.90e-01] [lr: 3.38e-05] [mem: 1.33e+04] (439.0 ms)
[67,   780] grad_stats: [6.01e-05 4.57e-05] (2.10e-05, 1.03e-03)
avg. loss 6.79032810e-03
time taken for epoch 0:05:55.956040
Epoch 68
[68,     0] loss: 7.072347e-03 masks: 20.0 16.0 [wd: 3.90e-01] [lr: 3.38e-05] [mem: 1.33e+04] (422.8 ms)
[68,     0] grad_stats: [1.38e-04 6.37e-05] (2.67e-05, 1.38e-03)
[68,   390] loss: 6.773901e-03 masks: 19.9 17.4 [wd: 3.91e-01] [lr: 2.98e-05] [mem: 1.33e+04] (436.0 ms)
[68,   390] grad_stats: [5.93e-05 4.85e-05] (2.31e-05, 1.07e-03)
[68,   780] loss: 6.783289e-03 masks: 20.3 17.2 [wd: 3.92e-01] [lr: 2.62e-05] [mem: 1.33e+04] (438.6 ms)
[68,   780] grad_stats: [8.42e-05 5.33e-05] (2.89e-05, 1.59e-03)
avg. loss 6.78328908e-03
time taken for epoch 0:05:55.739473
Epoch 69
[69,     0] loss: 7.111918e-03 masks: 23.0 16.0 [wd: 3.92e-01] [lr: 2.61e-05] [mem: 1.33e+04] (448.1 ms)
[69,     0] grad_stats: [9.48e-05 6.11e-05] (2.92e-05, 1.11e-03)
[69,   390] loss: 6.755458e-03 masks: 20.1 17.4 [wd: 3.93e-01] [lr: 2.27e-05] [mem: 1.33e+04] (437.4 ms)
[69,   390] grad_stats: [8.70e-05 4.91e-05] (2.24e-05, 1.46e-03)
[69,   780] loss: 6.739198e-03 masks: 20.2 17.3 [wd: 3.94e-01] [lr: 1.95e-05] [mem: 1.33e+04] (438.6 ms)
[69,   780] grad_stats: [1.00e-04 6.12e-05] (2.65e-05, 1.88e-03)
avg. loss 6.73919777e-03
time taken for epoch 0:05:55.714146
Epoch 70
[70,     0] loss: 7.228986e-03 masks: 17.0 16.0 [wd: 3.94e-01] [lr: 1.95e-05] [mem: 1.33e+04] (396.4 ms)
[70,     0] grad_stats: [1.30e-04 6.53e-05] (2.81e-05, 2.00e-03)
[70,   390] loss: 6.758446e-03 masks: 20.2 17.2 [wd: 3.95e-01] [lr: 1.66e-05] [mem: 1.33e+04] (437.4 ms)
[70,   390] grad_stats: [1.11e-04 7.25e-05] (2.34e-05, 1.42e-03)
[70,   780] loss: 6.746312e-03 masks: 20.3 17.2 [wd: 3.96e-01] [lr: 1.39e-05] [mem: 1.33e+04] (438.0 ms)
[70,   780] grad_stats: [1.12e-04 7.12e-05] (2.93e-05, 1.23e-03)
avg. loss 6.74631192e-03
time taken for epoch 0:05:55.298973
Epoch 71
[71,     0] loss: 6.071558e-03 masks: 16.0 20.0 [wd: 3.96e-01] [lr: 1.39e-05] [mem: 1.33e+04] (406.0 ms)
[71,     0] grad_stats: [8.50e-05 4.44e-05] (2.05e-05, 1.16e-03)
[71,   390] loss: 6.697686e-03 masks: 20.0 17.3 [wd: 3.97e-01] [lr: 1.14e-05] [mem: 1.33e+04] (436.4 ms)
[71,   390] grad_stats: [7.16e-05 5.27e-05] (2.12e-05, 1.67e-03)
[71,   780] loss: 6.711575e-03 masks: 20.2 17.2 [wd: 3.97e-01] [lr: 9.26e-06] [mem: 1.33e+04] (438.2 ms)
[71,   780] grad_stats: [8.64e-05 6.12e-05] (2.75e-05, 1.54e-03)
avg. loss 6.71157495e-03
time taken for epoch 0:05:55.353363
Epoch 72
[72,     0] loss: 7.536562e-03 masks: 20.0 16.0 [wd: 3.97e-01] [lr: 9.26e-06] [mem: 1.33e+04] (422.8 ms)
[72,     0] grad_stats: [1.48e-04 5.38e-05] (3.00e-05, 1.94e-03)
[72,   390] loss: 6.701026e-03 masks: 20.0 17.3 [wd: 3.98e-01] [lr: 7.33e-06] [mem: 1.33e+04] (436.5 ms)
[72,   390] grad_stats: [1.36e-04 4.30e-05] (2.33e-05, 1.27e-03)
[72,   780] loss: 6.711650e-03 masks: 20.5 17.1 [wd: 3.99e-01] [lr: 5.65e-06] [mem: 1.33e+04] (439.8 ms)
[72,   780] grad_stats: [1.59e-04 6.68e-05] (3.36e-05, 1.60e-03)
avg. loss 6.71165045e-03
time taken for epoch 0:05:56.584334
Epoch 73
[73,     0] loss: 5.999923e-03 masks: 17.0 20.0 [wd: 3.99e-01] [lr: 5.65e-06] [mem: 1.33e+04] (419.6 ms)
[73,     0] grad_stats: [6.69e-05 4.58e-05] (2.36e-05, 1.01e-03)
[73,   390] loss: 6.686500e-03 masks: 20.4 17.1 [wd: 3.99e-01] [lr: 4.23e-06] [mem: 1.33e+04] (439.2 ms)
[73,   390] grad_stats: [9.62e-05 7.12e-05] (2.58e-05, 1.51e-03)
[73,   780] loss: 6.686511e-03 masks: 20.4 17.2 [wd: 3.99e-01] [lr: 3.07e-06] [mem: 1.33e+04] (439.2 ms)
[73,   780] grad_stats: [9.36e-05 6.19e-05] (2.83e-05, 1.29e-03)
avg. loss 6.68651077e-03
time taken for epoch 0:05:56.168800
Epoch 74
[74,     0] loss: 5.791407e-03 masks: 19.0 20.0 [wd: 3.99e-01] [lr: 3.07e-06] [mem: 1.33e+04] (438.4 ms)
[74,     0] grad_stats: [7.53e-05 4.56e-05] (2.17e-05, 1.00e-03)
[74,   390] loss: 6.646633e-03 masks: 20.5 17.2 [wd: 4.00e-01] [lr: 2.16e-06] [mem: 1.33e+04] (440.5 ms)
[74,   390] grad_stats: [1.04e-04 4.58e-05] (2.42e-05, 1.47e-03)
[74,   780] loss: 6.643868e-03 masks: 20.4 17.3 [wd: 4.00e-01] [lr: 1.52e-06] [mem: 1.33e+04] (439.4 ms)
[74,   780] grad_stats: [1.33e-04 9.90e-05] (3.05e-05, 1.24e-03)
avg. loss 6.64386754e-03
time taken for epoch 0:05:56.269997
Epoch 75
[75,     0] loss: 6.534504e-03 masks: 24.0 16.0 [wd: 4.00e-01] [lr: 1.52e-06] [mem: 1.33e+04] (454.7 ms)
[75,     0] grad_stats: [9.12e-05 6.60e-05] (2.70e-05, 1.42e-03)
[75,   390] loss: 6.670937e-03 masks: 20.2 17.2 [wd: 4.00e-01] [lr: 1.13e-06] [mem: 1.33e+04] (437.3 ms)
[75,   390] grad_stats: [6.30e-05 5.23e-05] (2.46e-05, 1.14e-03)
[75,   780] loss: 6.641172e-03 masks: 20.1 17.3 [wd: 4.00e-01] [lr: 1.00e-06] [mem: 1.33e+04] (437.4 ms)
[75,   780] grad_stats: [8.15e-05 4.92e-05] (2.38e-05, 1.66e-03)
avg. loss 6.64117172e-03
time taken for epoch 0:05:54.762450
Total pretraining time 7:24:42.102087
