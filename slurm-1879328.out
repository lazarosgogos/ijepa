INFO:root:called-params configs/in100_vitb16_ep300_bs448.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 448,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 150,
                   'folder': 'logs_IN100/in100-vitb16-l2-ep300-bs448-w80',
                   'logging_frequency': 3,
                   'output_file': 'oin100-pretrain-vitb-l2-ep300-bs448-warmup80-seed0.out',
                   'write_tag': 'jepa-in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 on IMAGENET100 seed 0 batch size 448 wamup steps '
               '80',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 1,
                        'ema': [0.996, 1.0],
                        'epochs': 300,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
CRITICAL:root:PRETRAINING
{   'data': {   'batch_size': 448,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 150,
                   'folder': 'logs_IN100/in100-vitb16-l2-ep300-bs448-w80',
                   'logging_frequency': 3,
                   'output_file': 'oin100-pretrain-vitb-l2-ep300-bs448-warmup80-seed0.out',
                   'write_tag': 'jepa-in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 on IMAGENET100 seed 0 batch size 448 wamup steps '
               '80',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 1,
                        'ema': [0.996, 1.0],
                        'epochs': 300,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
CRITICAL:root:PRETRAINING
{   'data': {   'batch_size': 448,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 150,
                   'folder': 'logs_IN100/in100-vitb16-l2-ep300-bs448-w80',
                   'logging_frequency': 3,
                   'output_file': 'oin100-pretrain-vitb-l2-ep300-bs448-warmup80-seed0.out',
                   'write_tag': 'jepa-in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 on IMAGENET100 seed 0 batch size 448 wamup steps '
               '80',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 1,
                        'ema': [0.996, 1.0],
                        'epochs': 300,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
CRITICAL:root:PRETRAINING
{   'data': {   'batch_size': 448,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 150,
                   'folder': 'logs_IN100/in100-vitb16-l2-ep300-bs448-w80',
                   'logging_frequency': 3,
                   'output_file': 'oin100-pretrain-vitb-l2-ep300-bs448-warmup80-seed0.out',
                   'write_tag': 'jepa-in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 on IMAGENET100 seed 0 batch size 448 wamup steps '
               '80',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 1,
                        'ema': [0.996, 1.0],
                        'epochs': 300,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
INFO:root:Running... (rank: 0/4)
CRITICAL:root:PRETRAINING
INFO:root:Initialized (rank/world-size) 0/4
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/imagenet100/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:data-path datasets/imagenet100/train/
INFO:root:Initialized ImageNet
INFO:root:min. labeled indices 1300
INFO:root:ImageNet supervised dataset created
INFO:root:ImageNet supervised data loader created
INFO:root:data-path datasets/imagenet100/val/
INFO:root:Initialized ImageNet
INFO:root:min. labeled indices 50
INFO:root:ImageNet supervised dataset created
INFO:root:ImageNet supervised data loader created
INFO:root:Using AdamW
INFO:root:Epoch 1
INFO:root:[1,     0] loss: 4.705245e-01 masks: 49.0 30.0 [wd: 4.00e-02] [lr: 2.00e-04] [mem: 2.72e+04] (12712.8 ms)
INFO:root:[1,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[1,    24] loss: 2.464225e-01 masks: 46.8 32.9 [wd: 4.00e-02] [lr: 2.03e-04] [mem: 3.26e+04] (1448.7 ms)
INFO:root:[1,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[1,    48] loss: 1.870735e-01 masks: 46.2 33.3 [wd: 4.00e-02] [lr: 2.07e-04] [mem: 3.26e+04] (1363.6 ms)
INFO:root:[1,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 1.63920448e-01
INFO:root:time taken for epoch 0:03:16.671794
INFO:root:Epoch 2
INFO:root:[2,     0] loss: 1.162990e-01 masks: 55.0 30.0 [wd: 4.00e-02] [lr: 2.10e-04] [mem: 3.26e+04] (823.4 ms)
INFO:root:[2,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[2,    24] loss: 1.169860e-01 masks: 45.5 33.1 [wd: 4.00e-02] [lr: 2.13e-04] [mem: 3.26e+04] (706.3 ms)
INFO:root:[2,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[2,    48] loss: 1.146487e-01 masks: 46.6 32.7 [wd: 4.00e-02] [lr: 2.17e-04] [mem: 3.26e+04] (700.2 ms)
INFO:root:[2,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 1.11548009e-01
INFO:root:time taken for epoch 0:03:12.730861
INFO:root:Epoch 3
INFO:root:[3,     0] loss: 9.927797e-02 masks: 53.0 30.0 [wd: 4.00e-02] [lr: 2.20e-04] [mem: 3.26e+04] (996.6 ms)
INFO:root:[3,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[3,    24] loss: 1.011501e-01 masks: 43.9 34.4 [wd: 4.01e-02] [lr: 2.23e-04] [mem: 3.26e+04] (834.4 ms)
INFO:root:[3,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[3,    48] loss: 9.758917e-02 masks: 44.3 33.9 [wd: 4.01e-02] [lr: 2.27e-04] [mem: 3.26e+04] (776.8 ms)
INFO:root:[3,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 9.46813961e-02
INFO:root:time taken for epoch 0:03:17.097171
INFO:root:Epoch 4
INFO:root:[4,     0] loss: 8.683705e-02 masks: 46.0 35.0 [wd: 4.01e-02] [lr: 2.30e-04] [mem: 3.26e+04] (713.0 ms)
INFO:root:[4,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[4,    24] loss: 8.550535e-02 masks: 47.6 32.7 [wd: 4.01e-02] [lr: 2.33e-04] [mem: 3.26e+04] (711.6 ms)
INFO:root:[4,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[4,    48] loss: 8.563963e-02 masks: 45.8 33.4 [wd: 4.01e-02] [lr: 2.37e-04] [mem: 3.26e+04] (701.5 ms)
INFO:root:[4,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 8.47091359e-02
INFO:root:time taken for epoch 0:03:20.246607
INFO:root:Epoch 5
INFO:root:[5,     0] loss: 7.701425e-02 masks: 48.0 35.0 [wd: 4.02e-02] [lr: 2.40e-04] [mem: 3.26e+04] (727.0 ms)
INFO:root:[5,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[5,    24] loss: 7.768368e-02 masks: 45.6 33.8 [wd: 4.02e-02] [lr: 2.43e-04] [mem: 3.26e+04] (721.1 ms)
INFO:root:[5,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[5,    48] loss: 7.503873e-02 masks: 45.7 33.9 [wd: 4.02e-02] [lr: 2.47e-04] [mem: 3.31e+04] (719.9 ms)
INFO:root:[5,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 7.25923710e-02
INFO:root:time taken for epoch 0:03:31.707584
INFO:root:Epoch 6
INFO:root:[6,     0] loss: 6.537060e-02 masks: 52.0 30.0 [wd: 4.02e-02] [lr: 2.50e-04] [mem: 3.31e+04] (773.2 ms)
INFO:root:[6,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[6,    24] loss: 6.406014e-02 masks: 47.0 33.2 [wd: 4.03e-02] [lr: 2.53e-04] [mem: 3.31e+04] (696.2 ms)
INFO:root:[6,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[6,    48] loss: 6.207580e-02 masks: 46.4 33.3 [wd: 4.03e-02] [lr: 2.57e-04] [mem: 3.31e+04] (698.3 ms)
INFO:root:[6,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 6.07784225e-02
INFO:root:time taken for epoch 0:03:12.416071
INFO:root:Epoch 7
INFO:root:[7,     0] loss: 6.090512e-02 masks: 31.0 36.0 [wd: 4.04e-02] [lr: 2.60e-04] [mem: 3.31e+04] (771.4 ms)
INFO:root:[7,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[7,    24] loss: 5.594498e-02 masks: 43.4 34.1 [wd: 4.04e-02] [lr: 2.63e-04] [mem: 3.31e+04] (879.0 ms)
INFO:root:[7,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[7,    48] loss: 5.384759e-02 masks: 44.6 33.8 [wd: 4.04e-02] [lr: 2.67e-04] [mem: 3.31e+04] (1115.6 ms)
INFO:root:[7,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 5.23098483e-02
INFO:root:time taken for epoch 0:03:12.572991
INFO:root:Epoch 8
INFO:root:[8,     0] loss: 5.000579e-02 masks: 42.0 35.0 [wd: 4.05e-02] [lr: 2.70e-04] [mem: 3.31e+04] (684.3 ms)
INFO:root:[8,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[8,    24] loss: 4.913260e-02 masks: 45.5 33.4 [wd: 4.05e-02] [lr: 2.73e-04] [mem: 3.31e+04] (697.2 ms)
INFO:root:[8,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[8,    48] loss: 4.805910e-02 masks: 45.6 33.3 [wd: 4.06e-02] [lr: 2.77e-04] [mem: 3.31e+04] (696.7 ms)
INFO:root:[8,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 4.73928110e-02
INFO:root:time taken for epoch 0:03:14.770456
INFO:root:Epoch 9
INFO:root:[9,     0] loss: 5.063685e-02 masks: 49.0 30.0 [wd: 4.06e-02] [lr: 2.80e-04] [mem: 3.31e+04] (778.5 ms)
INFO:root:[9,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[9,    24] loss: 4.672420e-02 masks: 44.5 33.5 [wd: 4.07e-02] [lr: 2.83e-04] [mem: 3.31e+04] (703.7 ms)
INFO:root:[9,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[9,    48] loss: 4.606039e-02 masks: 44.2 33.6 [wd: 4.07e-02] [lr: 2.87e-04] [mem: 3.31e+04] (696.3 ms)
INFO:root:[9,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 4.55367617e-02
INFO:root:time taken for epoch 0:03:12.483775
INFO:root:Epoch 10
INFO:root:[10,     0] loss: 4.375389e-02 masks: 38.0 35.0 [wd: 4.08e-02] [lr: 2.90e-04] [mem: 3.31e+04] (800.3 ms)
INFO:root:[10,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[10,    24] loss: 4.245159e-02 masks: 47.8 32.1 [wd: 4.09e-02] [lr: 2.93e-04] [mem: 3.31e+04] (718.0 ms)
INFO:root:[10,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[10,    48] loss: 4.247365e-02 masks: 48.1 32.0 [wd: 4.09e-02] [lr: 2.97e-04] [mem: 3.31e+04] (708.5 ms)
INFO:root:[10,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 4.30194833e-02
INFO:root:time taken for epoch 0:03:11.662194
INFO:root:Epoch 11
INFO:root:[11,     0] loss: 4.261977e-02 masks: 45.0 35.0 [wd: 4.10e-02] [lr: 3.00e-04] [mem: 3.31e+04] (1156.1 ms)
INFO:root:[11,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[11,    24] loss: 4.412297e-02 masks: 44.7 33.6 [wd: 4.11e-02] [lr: 3.03e-04] [mem: 3.31e+04] (756.1 ms)
INFO:root:[11,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[11,    48] loss: 4.518236e-02 masks: 44.2 34.0 [wd: 4.11e-02] [lr: 3.07e-04] [mem: 3.31e+04] (745.2 ms)
INFO:root:[11,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 4.53270784e-02
INFO:root:time taken for epoch 0:03:26.001150
INFO:root:Epoch 12
INFO:root:[12,     0] loss: 4.136110e-02 masks: 56.0 30.0 [wd: 4.12e-02] [lr: 3.10e-04] [mem: 3.31e+04] (771.0 ms)
INFO:root:[12,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[12,    24] loss: 4.455502e-02 masks: 48.4 32.9 [wd: 4.13e-02] [lr: 3.13e-04] [mem: 3.31e+04] (691.8 ms)
INFO:root:[12,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[12,    48] loss: 4.544264e-02 masks: 47.3 33.1 [wd: 4.13e-02] [lr: 3.17e-04] [mem: 3.31e+04] (688.7 ms)
INFO:root:[12,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 4.56998547e-02
INFO:root:time taken for epoch 0:03:18.979086
INFO:root:Epoch 13
INFO:root:[13,     0] loss: 4.359169e-02 masks: 46.0 35.0 [wd: 4.14e-02] [lr: 3.20e-04] [mem: 3.31e+04] (11498.4 ms)
INFO:root:[13,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[13,    24] loss: 4.722199e-02 masks: 44.5 34.0 [wd: 4.15e-02] [lr: 3.23e-04] [mem: 3.31e+04] (1924.6 ms)
INFO:root:[13,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[13,    48] loss: 4.756966e-02 masks: 45.6 33.6 [wd: 4.16e-02] [lr: 3.27e-04] [mem: 3.31e+04] (1813.9 ms)
INFO:root:[13,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 4.77146673e-02
INFO:root:time taken for epoch 0:03:17.553257
INFO:root:Epoch 14
INFO:root:[14,     0] loss: 4.687290e-02 masks: 49.0 30.0 [wd: 4.17e-02] [lr: 3.30e-04] [mem: 3.31e+04] (709.1 ms)
INFO:root:[14,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[14,    24] loss: 4.908792e-02 masks: 46.6 32.8 [wd: 4.18e-02] [lr: 3.33e-04] [mem: 3.31e+04] (756.1 ms)
INFO:root:[14,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[14,    48] loss: 5.050738e-02 masks: 45.4 33.5 [wd: 4.18e-02] [lr: 3.37e-04] [mem: 3.31e+04] (727.4 ms)
INFO:root:[14,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 5.12476415e-02
INFO:root:time taken for epoch 0:03:14.826416
INFO:root:Epoch 15
INFO:root:[15,     0] loss: 5.512125e-02 masks: 25.0 36.0 [wd: 4.19e-02] [lr: 3.40e-04] [mem: 3.31e+04] (647.7 ms)
INFO:root:[15,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[15,    24] loss: 5.312676e-02 masks: 46.0 33.2 [wd: 4.20e-02] [lr: 3.43e-04] [mem: 3.31e+04] (702.2 ms)
INFO:root:[15,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[15,    48] loss: 5.371030e-02 masks: 45.4 33.5 [wd: 4.21e-02] [lr: 3.47e-04] [mem: 3.31e+04] (694.1 ms)
INFO:root:[15,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 5.43480626e-02
INFO:root:time taken for epoch 0:03:14.667804
INFO:root:Epoch 16
INFO:root:[16,     0] loss: 6.056985e-02 masks: 43.0 36.0 [wd: 4.22e-02] [lr: 3.50e-04] [mem: 3.31e+04] (718.7 ms)
INFO:root:[16,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[16,    24] loss: 5.546734e-02 masks: 48.2 33.0 [wd: 4.23e-02] [lr: 3.53e-04] [mem: 3.31e+04] (723.8 ms)
INFO:root:[16,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[16,    48] loss: 5.704149e-02 masks: 47.1 33.0 [wd: 4.24e-02] [lr: 3.57e-04] [mem: 3.31e+04] (699.4 ms)
INFO:root:[16,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 5.70200266e-02
INFO:root:time taken for epoch 0:03:16.242748
INFO:root:Epoch 17
INFO:root:[17,     0] loss: 5.714602e-02 masks: 41.0 36.0 [wd: 4.25e-02] [lr: 3.60e-04] [mem: 3.31e+04] (2161.1 ms)
INFO:root:[17,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[17,    24] loss: 6.059425e-02 masks: 45.7 33.0 [wd: 4.26e-02] [lr: 3.63e-04] [mem: 3.31e+04] (971.8 ms)
INFO:root:[17,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[17,    48] loss: 6.198872e-02 masks: 45.1 33.2 [wd: 4.27e-02] [lr: 3.67e-04] [mem: 3.31e+04] (887.0 ms)
INFO:root:[17,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 6.24821120e-02
INFO:root:time taken for epoch 0:03:08.374784
INFO:root:Epoch 18
INFO:root:[18,     0] loss: 6.685156e-02 masks: 43.0 36.0 [wd: 4.28e-02] [lr: 3.70e-04] [mem: 3.40e+04] (638.6 ms)
INFO:root:[18,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[18,    24] loss: 6.352001e-02 masks: 48.2 32.4 [wd: 4.30e-02] [lr: 3.73e-04] [mem: 3.40e+04] (671.1 ms)
INFO:root:[18,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[18,    48] loss: 6.517695e-02 masks: 46.2 33.3 [wd: 4.31e-02] [lr: 3.77e-04] [mem: 3.40e+04] (670.0 ms)
INFO:root:[18,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 6.59259731e-02
INFO:root:time taken for epoch 0:03:23.486049
INFO:root:Epoch 19
INFO:root:[19,     0] loss: 6.814724e-02 masks: 49.0 30.0 [wd: 4.32e-02] [lr: 3.80e-04] [mem: 3.40e+04] (771.4 ms)
INFO:root:[19,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[19,    24] loss: 6.869648e-02 masks: 44.2 33.6 [wd: 4.33e-02] [lr: 3.83e-04] [mem: 3.40e+04] (701.8 ms)
INFO:root:[19,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[19,    48] loss: 6.832284e-02 masks: 46.5 33.0 [wd: 4.34e-02] [lr: 3.87e-04] [mem: 3.40e+04] (696.8 ms)
INFO:root:[19,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 6.97944316e-02
INFO:root:time taken for epoch 0:03:11.277577
INFO:root:Epoch 20
INFO:root:[20,     0] loss: 7.216830e-02 masks: 49.0 30.0 [wd: 4.36e-02] [lr: 3.90e-04] [mem: 3.40e+04] (637.5 ms)
INFO:root:[20,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[20,    24] loss: 7.173783e-02 masks: 47.2 33.1 [wd: 4.37e-02] [lr: 3.93e-04] [mem: 3.40e+04] (695.2 ms)
INFO:root:[20,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[20,    48] loss: 7.415925e-02 masks: 45.6 33.7 [wd: 4.38e-02] [lr: 3.97e-04] [mem: 3.40e+04] (689.1 ms)
INFO:root:[20,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 7.53893251e-02
INFO:root:time taken for epoch 0:03:18.330430
INFO:root:Epoch 21
INFO:root:[21,     0] loss: 7.756128e-02 masks: 45.0 35.0 [wd: 4.39e-02] [lr: 4.00e-04] [mem: 3.40e+04] (675.2 ms)
INFO:root:[21,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[21,    24] loss: 8.034712e-02 masks: 43.5 34.7 [wd: 4.41e-02] [lr: 4.03e-04] [mem: 3.40e+04] (664.4 ms)
INFO:root:[21,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[21,    48] loss: 8.084211e-02 masks: 42.7 34.5 [wd: 4.42e-02] [lr: 4.07e-04] [mem: 3.40e+04] (664.7 ms)
INFO:root:[21,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 8.06096461e-02
INFO:root:time taken for epoch 0:03:14.217239
INFO:root:Epoch 22
INFO:root:[22,     0] loss: 8.045938e-02 masks: 43.0 35.0 [wd: 4.43e-02] [lr: 4.10e-04] [mem: 3.40e+04] (854.4 ms)
INFO:root:[22,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[22,    24] loss: 8.222326e-02 masks: 44.6 33.6 [wd: 4.45e-02] [lr: 4.13e-04] [mem: 3.40e+04] (780.3 ms)
INFO:root:[22,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[22,    48] loss: 8.251300e-02 masks: 45.0 33.8 [wd: 4.46e-02] [lr: 4.17e-04] [mem: 3.40e+04] (848.5 ms)
INFO:root:[22,    48] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:avg. loss 8.28234006e-02
INFO:root:time taken for epoch 0:03:17.908419
INFO:root:Epoch 23
INFO:root:[23,     0] loss: 8.357771e-02 masks: 54.0 30.0 [wd: 4.48e-02] [lr: 4.20e-04] [mem: 3.40e+04] (777.0 ms)
INFO:root:[23,     0] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
INFO:root:[23,    24] loss: 8.644741e-02 masks: 44.8 34.0 [wd: 4.49e-02] [lr: 4.23e-04] [mem: 3.40e+04] (677.2 ms)
INFO:root:[23,    24] grad_stats: [0.00e+00 0.00e+00] (inf, -inf)
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f0886ce3560>
Traceback (most recent call last):
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1443, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1593883) is killed by signal: Aborted. 
Process Process-3:
Traceback (most recent call last):
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/l/lazarosg/thesis/ijepa/main.py", line 101, in process_main
    app_main(args=params)
  File "/home/l/lazarosg/thesis/ijepa/src/train.py", line 493, in main
    (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)
                                                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/utils/logging.py", line 21, in gpu_timer
    result = closure()
             ^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/train.py", line 463, in train_step
    z = forward_context()
        ^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/train.py", line 441, in forward_context
    z = predictor(z, masks_enc, masks_pred)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/models/vision_transformer.py", line 330, in forward
    x = blk(x)
        ^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/models/vision_transformer.py", line 173, in forward
    x = x + self.drop_path(self.mlp(self.norm2(x)))
                           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/models/vision_transformer.py", line 124, in forward
    x = self.fc2(x)
        ^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 114.31 MiB is free. Including non-PyTorch memory, this process has 39.25 GiB memory in use. Of the allocated memory 33.84 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
slurmstepd: error: *** JOB 1879328 ON cn50 CANCELLED AT 2025-01-24T05:08:11 DUE TO TIME LIMIT ***
