nohup: ignoring input
INFO:root:called-params configs/in.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet',
                'num_workers': 2,
                'pin_mem': False,
                'root_path': '/media/shared/imagenet',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {'folder': 'logs/in1k', 'write_tag': 'jepa_in1k'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.996, 1.0],
                        'epochs': 300,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 40,
                        'weight_decay': 0.04}}
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path /media/shared/imagenet/imagenet/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:Using AdamW
INFO:root:Epoch 1
INFO:root:[1,     0] loss: 4.664854e-01 masks: 69.0 42.0 [wd: 4.00e-02] [lr: 2.00e-04] [mem: 1.44e+04] (911.2 ms)
INFO:root:[1,     0] grad_stats: [2.93e-02 1.56e-02] (1.50e-02, 3.36e-02)
Process Process-1:
Traceback (most recent call last):
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lazarosg/ijepa/main.py", line 68, in process_main
    app_main(args=params)
  File "/home/lazarosg/ijepa/src/train.py", line 396, in main
    (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)
                                                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/ijepa/src/utils/logging.py", line 21, in gpu_timer
    result = closure()
             ^^^^^^^^^
  File "/home/lazarosg/ijepa/src/train.py", line 375, in train_step
    z = forward_context()
        ^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/ijepa/src/train.py", line 316, in forward_context
    z = predictor(z, masks_enc, masks_pred)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/ijepa/src/models/vision_transformer.py", line 319, in forward
    x = blk(x)
        ^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/ijepa/src/models/vision_transformer.py", line 166, in forward
    y, attn = self.attn(self.norm1(x))
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/miniconda3/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lazarosg/ijepa/src/models/vision_transformer.py", line 143, in forward
    attn = (q @ k.transpose(-2, -1)) * self.scale
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 170.00 MiB. GPU 
