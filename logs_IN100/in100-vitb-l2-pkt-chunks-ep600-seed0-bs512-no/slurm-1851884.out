INFO:root:called-params configs/in100_vitb16_ep600_pkt_bs512.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 512,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 300,
                   'folder': 'logs_IN100/in100-vitb-l2-pkt-chunks-ep600-seed0-bs512',
                   'logging_frequency': 2,
                   'output_file': 'oin100-pretrain-vitb-l2-pkt-chunks-ep600-seed0.out',
                   'write_tag': 'jepa_in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 + PKT chunks on IMAGENET100 seed 0',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 2,
                        'ema': [0.996, 1.0],
                        'epochs': 600,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2_PKT_chunks',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
CRITICAL:root:PRETRAINING
{   'data': {   'batch_size': 512,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 300,
                   'folder': 'logs_IN100/in100-vitb-l2-pkt-chunks-ep600-seed0-bs512',
                   'logging_frequency': 2,
                   'output_file': 'oin100-pretrain-vitb-l2-pkt-chunks-ep600-seed0.out',
                   'write_tag': 'jepa_in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 + PKT chunks on IMAGENET100 seed 0',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 2,
                        'ema': [0.996, 1.0],
                        'epochs': 600,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2_PKT_chunks',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
CRITICAL:root:PRETRAINING
{   'data': {   'batch_size': 512,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 300,
                   'folder': 'logs_IN100/in100-vitb-l2-pkt-chunks-ep600-seed0-bs512',
                   'logging_frequency': 2,
                   'output_file': 'oin100-pretrain-vitb-l2-pkt-chunks-ep600-seed0.out',
                   'write_tag': 'jepa_in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 + PKT chunks on IMAGENET100 seed 0',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 2,
                        'ema': [0.996, 1.0],
                        'epochs': 600,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2_PKT_chunks',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
CRITICAL:root:PRETRAINING
{   'data': {   'batch_size': 512,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'imagenet100',
                'num_workers': 10,
                'pin_mem': True,
                'root_path': 'datasets/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'checkpoint_freq': 300,
                   'folder': 'logs_IN100/in100-vitb-l2-pkt-chunks-ep600-seed0-bs512',
                   'logging_frequency': 2,
                   'output_file': 'oin100-pretrain-vitb-l2-pkt-chunks-ep600-seed0.out',
                   'write_tag': 'jepa_in100'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 16,
                'pred_mask_scale': [0.15, 0.2]},
    'message': 'ViT-B/16 L2 + PKT chunks on IMAGENET100 seed 0',
    'meta': {   'copy_data': False,
                'load_checkpoint': False,
                'model_name': 'vit_base',
                'pred_depth': 6,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'accumulate_grads_every': 2,
                        'ema': [0.996, 1.0],
                        'epochs': 600,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'loss_function': 'L2_PKT_chunks',
                        'lr': 0.001,
                        'pkt_scale': 1.0,
                        'start_lr': 0.0002,
                        'variance_weight': 0.0,
                        'warmup': 80,
                        'weight_decay': 0.04},
    'pkt': {   'T_max': 200,
               'chunks_step': 256,
               'final_alpha': 0.0,
               'ref_alpha': 1.0,
               'start_alpha': 1.0,
               'use_pkt_scheduler': False,
               'warmup_steps_alpha': 100}}
INFO:root:Running... (rank: 0/4)
CRITICAL:root:PRETRAINING
INFO:root:Initialized (rank/world-size) 0/4
INFO:root:train.py: _GLOBAL_SEED=0
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path datasets/imagenet100/train/
INFO:root:Initialized ImageNet
INFO:root:ImageNet dataset created
INFO:root:ImageNet unsupervised data loader created
INFO:root:data-path datasets/imagenet100/train/
INFO:root:Initialized ImageNet
INFO:root:min. labeled indices 1300
INFO:root:ImageNet supervised dataset created
INFO:root:ImageNet supervised data loader created
INFO:root:data-path datasets/imagenet100/val/
INFO:root:Initialized ImageNet
INFO:root:min. labeled indices 50
INFO:root:ImageNet supervised dataset created
INFO:root:ImageNet supervised data loader created
INFO:root:Using AdamW
INFO:root:Epoch 1
INFO:root:[1,     0] loss: 8.824331e-04 masks: 37.0 36.0 [wd: 4.00e-02] [lr: 2.00e-04] [mem: 2.73e+04] (16733.8 ms)
INFO:root:[1,     0] grad_stats: [2.33e+00 1.21e+00] (1.21e+00, 2.77e+00)
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1fce83f560>
Traceback (most recent call last):
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1443, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1109404) is killed by signal: Aborted. 
Process Process-1:
Traceback (most recent call last):
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/l/lazarosg/thesis/ijepa/main.py", line 101, in process_main
    app_main(args=params)
  File "/home/l/lazarosg/thesis/ijepa/src/train.py", line 493, in main
    (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)
                                                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/utils/logging.py", line 21, in gpu_timer
    result = closure()
             ^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/train.py", line 463, in train_step
    z = forward_context()
        ^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/train.py", line 441, in forward_context
    z = predictor(z, masks_enc, masks_pred)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/models/vision_transformer.py", line 330, in forward
    x = blk(x)
        ^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/models/vision_transformer.py", line 173, in forward
    x = x + self.drop_path(self.mlp(self.norm2(x)))
                           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/thesis/ijepa/src/models/vision_transformer.py", line 121, in forward
    x = self.fc1(x)
        ^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 522.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 6.31 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 33.65 GiB is allocated by PyTorch, and 4.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]:[E ProcessGroupNCCL.cpp:523] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=24, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600113 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1182] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=24, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600113 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1708025824022/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f2c59a43d87 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f2c5abdcd26 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f2c5abe027d in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f2c5abe0e79 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbbf4 (0x7f2cb7794bf4 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x89c02 (0x7f2cbfa89c02 in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10ec40 (0x7f2cbfb0ec40 in /lib64/libc.so.6)

[rank2]:[E ProcessGroupNCCL.cpp:523] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=24, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600963 milliseconds before timing out.
[rank2]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:1182] [Rank 2] NCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=24, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600963 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1708025824022/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7ff5d43aad87 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7ff5d5543d26 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7ff5d554727d in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7ff5d5547e79 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbbf4 (0x7ff6320fbbf4 in /home/l/lazarosg/.conda/envs/ijepa/lib/python3.12/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x89c02 (0x7ff63a489c02 in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10ec40 (0x7ff63a50ec40 in /lib64/libc.so.6)

slurmstepd: error: *** JOB 1851884 ON cn50 CANCELLED AT 2024-11-25T08:56:30 ***
