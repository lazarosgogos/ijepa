VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/intel-image-classification/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded optimizers from epoch 99
read-path: logs_PKT/iic-train-PKT-seed10-maxvar100/jepa-iic-PKT-seed10-maxvar100-ep100.pth.tar
Starting
Iteration: 0
torch.Size([64, 4, 16, 768])
torch.Size([4, 16, 768])
Iteration: 1
time taken for epoch 0:01:46.133097
Total pretraining time 0:01:46.168716
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/intel-image-classification/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded optimizers from epoch 99
read-path: logs_PKT/iic-train-PKT-seed10-maxvar100/jepa-iic-PKT-seed10-maxvar100-ep100.pth.tar
Starting
Iteration: 0
torch.Size([64, 4, 16, 768])
torch.Size([4, 16, 768])
Iteration: 1
time taken for epoch 0:01:44.658867
Total pretraining time 0:01:44.658974
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/intel-image-classification/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/intel-image-classification/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded optimizers from epoch 99
read-path: logs_PKT/iic-train-PKT-seed10-maxvar100/jepa-iic-PKT-seed10-maxvar100-ep100.pth.tar
Starting
Iteration: 0
z[:100] values: tensor([[[ 8.0549e+00, -1.0782e-02,  1.8770e+01,  ..., -1.2024e-02,
          -2.7712e-01, -1.6306e+00],
         [ 8.0550e+00, -1.0820e-02,  1.8769e+01,  ..., -1.1970e-02,
          -2.7711e-01, -1.6306e+00],
         [ 8.0550e+00, -1.0849e-02,  1.8769e+01,  ..., -1.1959e-02,
          -2.7712e-01, -1.6306e+00],
         ...,
         [ 8.0553e+00, -1.0973e-02,  1.8769e+01,  ..., -1.1836e-02,
          -2.7690e-01, -1.6303e+00],
         [ 8.0553e+00, -1.1001e-02,  1.8769e+01,  ..., -1.1829e-02,
          -2.7689e-01, -1.6304e+00],
         [ 8.0553e+00, -1.1013e-02,  1.8769e+01,  ..., -1.1876e-02,
          -2.7687e-01, -1.6304e+00]],

        [[ 8.0509e+00, -1.1044e-02,  1.8773e+01,  ..., -1.2656e-02,
          -2.7581e-01, -1.6344e+00],
         [ 8.0510e+00, -1.1074e-02,  1.8773e+01,  ..., -1.2565e-02,
          -2.7584e-01, -1.6345e+00],
         [ 8.0514e+00, -1.1133e-02,  1.8773e+01,  ..., -1.2499e-02,
          -2.7596e-01, -1.6344e+00],
         ...,
         [ 8.0529e+00, -1.1212e-02,  1.8772e+01,  ..., -1.2625e-02,
          -2.7645e-01, -1.6338e+00],
         [ 8.0530e+00, -1.1251e-02,  1.8772e+01,  ..., -1.2567e-02,
          -2.7646e-01, -1.6338e+00],
         [ 8.0531e+00, -1.1276e-02,  1.8772e+01,  ..., -1.2549e-02,
          -2.7648e-01, -1.6338e+00]],

        [[ 8.0540e+00, -1.0309e-02,  1.8768e+01,  ..., -1.1822e-02,
          -2.7697e-01, -1.6288e+00],
         [ 8.0540e+00, -1.0324e-02,  1.8768e+01,  ..., -1.1753e-02,
          -2.7695e-01, -1.6289e+00],
         [ 8.0540e+00, -1.0347e-02,  1.8768e+01,  ..., -1.1680e-02,
          -2.7692e-01, -1.6289e+00],
         ...,
         [ 8.0542e+00, -1.0401e-02,  1.8768e+01,  ..., -1.1565e-02,
          -2.7674e-01, -1.6288e+00],
         [ 8.0541e+00, -1.0429e-02,  1.8768e+01,  ..., -1.1492e-02,
          -2.7671e-01, -1.6288e+00],
         [ 8.0541e+00, -1.0460e-02,  1.8768e+01,  ..., -1.1438e-02,
          -2.7670e-01, -1.6289e+00]],

        ...,

        [[ 8.0554e+00, -1.1082e-02,  1.8770e+01,  ..., -1.1963e-02,
          -2.7765e-01, -1.6323e+00],
         [ 8.0554e+00, -1.1119e-02,  1.8770e+01,  ..., -1.1902e-02,
          -2.7762e-01, -1.6324e+00],
         [ 8.0554e+00, -1.1160e-02,  1.8770e+01,  ..., -1.1836e-02,
          -2.7758e-01, -1.6324e+00],
         ...,
         [ 8.0547e+00, -1.0909e-02,  1.8770e+01,  ..., -1.1936e-02,
          -2.7749e-01, -1.6315e+00],
         [ 8.0555e+00, -1.1037e-02,  1.8770e+01,  ..., -1.1924e-02,
          -2.7758e-01, -1.6318e+00],
         [ 8.0559e+00, -1.1117e-02,  1.8770e+01,  ..., -1.1901e-02,
          -2.7762e-01, -1.6319e+00]],

        [[ 8.0511e+00, -1.1333e-02,  1.8775e+01,  ..., -1.2454e-02,
          -2.7614e-01, -1.6355e+00],
         [ 8.0514e+00, -1.1384e-02,  1.8775e+01,  ..., -1.2386e-02,
          -2.7625e-01, -1.6355e+00],
         [ 8.0519e+00, -1.1453e-02,  1.8774e+01,  ..., -1.2390e-02,
          -2.7634e-01, -1.6354e+00],
         ...,
         [ 8.0526e+00, -1.1487e-02,  1.8774e+01,  ..., -1.2454e-02,
          -2.7659e-01, -1.6351e+00],
         [ 8.0527e+00, -1.1516e-02,  1.8774e+01,  ..., -1.2440e-02,
          -2.7661e-01, -1.6351e+00],
         [ 8.0527e+00, -1.1529e-02,  1.8774e+01,  ..., -1.2477e-02,
          -2.7661e-01, -1.6351e+00]],

        [[ 8.0537e+00, -1.0747e-02,  1.8770e+01,  ..., -1.1981e-02,
          -2.7678e-01, -1.6312e+00],
         [ 8.0537e+00, -1.0778e-02,  1.8770e+01,  ..., -1.1971e-02,
          -2.7678e-01, -1.6312e+00],
         [ 8.0538e+00, -1.0789e-02,  1.8770e+01,  ..., -1.2011e-02,
          -2.7678e-01, -1.6312e+00],
         ...,
         [ 8.0539e+00, -1.0863e-02,  1.8769e+01,  ..., -1.1788e-02,
          -2.7657e-01, -1.6311e+00],
         [ 8.0540e+00, -1.0873e-02,  1.8769e+01,  ..., -1.1827e-02,
          -2.7656e-01, -1.6310e+00],
         [ 8.0540e+00, -1.0863e-02,  1.8769e+01,  ..., -1.1896e-02,
          -2.7654e-01, -1.6310e+00]]], device='cuda:0')
torch.Size([64, 4, 16, 768])
torch.Size([4, 16, 768])
Iteration: 1
time taken for epoch 0:03:55.279821
Total pretraining time 0:03:58.367971
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/intel-image-classification/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded optimizers from epoch 99
read-path: logs_PKT/iic-train-PKT-seed10-maxvar100/jepa-iic-PKT-seed10-maxvar100-ep100.pth.tar
Starting
Iteration: 0
z[0] values: tensor([[ 8.0549e+00, -1.0782e-02,  1.8770e+01,  ..., -1.2024e-02,
         -2.7712e-01, -1.6306e+00],
        [ 8.0550e+00, -1.0820e-02,  1.8769e+01,  ..., -1.1970e-02,
         -2.7711e-01, -1.6306e+00],
        [ 8.0550e+00, -1.0849e-02,  1.8769e+01,  ..., -1.1959e-02,
         -2.7712e-01, -1.6306e+00],
        ...,
        [ 8.0553e+00, -1.0973e-02,  1.8769e+01,  ..., -1.1836e-02,
         -2.7690e-01, -1.6303e+00],
        [ 8.0553e+00, -1.1001e-02,  1.8769e+01,  ..., -1.1829e-02,
         -2.7689e-01, -1.6304e+00],
        [ 8.0553e+00, -1.1013e-02,  1.8769e+01,  ..., -1.1876e-02,
         -2.7687e-01, -1.6304e+00]], device='cuda:0')
torch.Size([64, 4, 16, 768])
torch.Size([4, 16, 768])
Iteration: 1
time taken for epoch 0:02:31.508582
Total pretraining time 0:02:32.022007
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/intel-image-classification/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded optimizers from epoch 99
read-path: logs_PKT/iic-train-PKT-seed10-maxvar100/jepa-iic-PKT-seed10-maxvar100-ep100.pth.tar
Starting
Iteration: 0
z[:100] values: tensor([[[ 8.0549e+00, -1.0782e-02,  1.8770e+01,  ..., -1.2024e-02,
          -2.7712e-01, -1.6306e+00],
         [ 8.0550e+00, -1.0820e-02,  1.8769e+01,  ..., -1.1970e-02,
          -2.7711e-01, -1.6306e+00],
         [ 8.0550e+00, -1.0849e-02,  1.8769e+01,  ..., -1.1959e-02,
          -2.7712e-01, -1.6306e+00],
         ...,
         [ 8.0553e+00, -1.0973e-02,  1.8769e+01,  ..., -1.1836e-02,
          -2.7690e-01, -1.6303e+00],
         [ 8.0553e+00, -1.1001e-02,  1.8769e+01,  ..., -1.1829e-02,
          -2.7689e-01, -1.6304e+00],
         [ 8.0553e+00, -1.1013e-02,  1.8769e+01,  ..., -1.1876e-02,
          -2.7687e-01, -1.6304e+00]],

        [[ 8.0509e+00, -1.1044e-02,  1.8773e+01,  ..., -1.2656e-02,
          -2.7581e-01, -1.6344e+00],
         [ 8.0510e+00, -1.1074e-02,  1.8773e+01,  ..., -1.2565e-02,
          -2.7584e-01, -1.6345e+00],
         [ 8.0514e+00, -1.1133e-02,  1.8773e+01,  ..., -1.2499e-02,
          -2.7596e-01, -1.6344e+00],
         ...,
         [ 8.0529e+00, -1.1212e-02,  1.8772e+01,  ..., -1.2625e-02,
          -2.7645e-01, -1.6338e+00],
         [ 8.0530e+00, -1.1251e-02,  1.8772e+01,  ..., -1.2567e-02,
          -2.7646e-01, -1.6338e+00],
         [ 8.0531e+00, -1.1276e-02,  1.8772e+01,  ..., -1.2549e-02,
          -2.7648e-01, -1.6338e+00]],

        [[ 8.0540e+00, -1.0309e-02,  1.8768e+01,  ..., -1.1822e-02,
          -2.7697e-01, -1.6288e+00],
         [ 8.0540e+00, -1.0324e-02,  1.8768e+01,  ..., -1.1753e-02,
          -2.7695e-01, -1.6289e+00],
         [ 8.0540e+00, -1.0347e-02,  1.8768e+01,  ..., -1.1680e-02,
          -2.7692e-01, -1.6289e+00],
         ...,
         [ 8.0542e+00, -1.0401e-02,  1.8768e+01,  ..., -1.1565e-02,
          -2.7674e-01, -1.6288e+00],
         [ 8.0541e+00, -1.0429e-02,  1.8768e+01,  ..., -1.1492e-02,
          -2.7671e-01, -1.6288e+00],
         [ 8.0541e+00, -1.0460e-02,  1.8768e+01,  ..., -1.1438e-02,
          -2.7670e-01, -1.6289e+00]],

        ...,

        [[ 8.0554e+00, -1.1082e-02,  1.8770e+01,  ..., -1.1963e-02,
          -2.7765e-01, -1.6323e+00],
         [ 8.0554e+00, -1.1119e-02,  1.8770e+01,  ..., -1.1902e-02,
          -2.7762e-01, -1.6324e+00],
         [ 8.0554e+00, -1.1160e-02,  1.8770e+01,  ..., -1.1836e-02,
          -2.7758e-01, -1.6324e+00],
         ...,
         [ 8.0547e+00, -1.0909e-02,  1.8770e+01,  ..., -1.1936e-02,
          -2.7749e-01, -1.6315e+00],
         [ 8.0555e+00, -1.1037e-02,  1.8770e+01,  ..., -1.1924e-02,
          -2.7758e-01, -1.6318e+00],
         [ 8.0559e+00, -1.1117e-02,  1.8770e+01,  ..., -1.1901e-02,
          -2.7762e-01, -1.6319e+00]],

        [[ 8.0511e+00, -1.1333e-02,  1.8775e+01,  ..., -1.2454e-02,
          -2.7614e-01, -1.6355e+00],
         [ 8.0514e+00, -1.1384e-02,  1.8775e+01,  ..., -1.2386e-02,
          -2.7625e-01, -1.6355e+00],
         [ 8.0519e+00, -1.1453e-02,  1.8774e+01,  ..., -1.2390e-02,
          -2.7634e-01, -1.6354e+00],
         ...,
         [ 8.0526e+00, -1.1487e-02,  1.8774e+01,  ..., -1.2454e-02,
          -2.7659e-01, -1.6351e+00],
         [ 8.0527e+00, -1.1516e-02,  1.8774e+01,  ..., -1.2440e-02,
          -2.7661e-01, -1.6351e+00],
         [ 8.0527e+00, -1.1529e-02,  1.8774e+01,  ..., -1.2477e-02,
          -2.7661e-01, -1.6351e+00]],

        [[ 8.0537e+00, -1.0747e-02,  1.8770e+01,  ..., -1.1981e-02,
          -2.7678e-01, -1.6312e+00],
         [ 8.0537e+00, -1.0778e-02,  1.8770e+01,  ..., -1.1971e-02,
          -2.7678e-01, -1.6312e+00],
         [ 8.0538e+00, -1.0789e-02,  1.8770e+01,  ..., -1.2011e-02,
          -2.7678e-01, -1.6312e+00],
         ...,
         [ 8.0539e+00, -1.0863e-02,  1.8769e+01,  ..., -1.1788e-02,
          -2.7657e-01, -1.6311e+00],
         [ 8.0540e+00, -1.0873e-02,  1.8769e+01,  ..., -1.1827e-02,
          -2.7656e-01, -1.6310e+00],
         [ 8.0540e+00, -1.0863e-02,  1.8769e+01,  ..., -1.1896e-02,
          -2.7654e-01, -1.6310e+00]]], device='cuda:0')
torch.Size([64, 4, 16, 768])
torch.Size([4, 16, 768])
Iteration: 1
time taken for epoch 0:01:41.269520
Total pretraining time 0:01:41.269627
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(15, 15), stride=(15, 15))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
making imagenet data transforms
data-path datasets/intel-image-classification/train/
Initialized ImageNet
ImageNet dataset created
ImageNet unsupervised data loader created
Using AdamW
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded pretrained encoder from epoch 99 with msg: <All keys matched successfully>
loaded optimizers from epoch 99
read-path: logs_PKT/iic-train-PKT-seed10-maxvar100/jepa-iic-PKT-seed10-maxvar100-ep100.pth.tar
Starting
Iteration: 0
z[:100] values: tensor([[[ 8.0549e+00, -1.0782e-02,  1.8770e+01,  ..., -1.2024e-02,
          -2.7712e-01, -1.6306e+00],
         [ 8.0550e+00, -1.0820e-02,  1.8769e+01,  ..., -1.1970e-02,
          -2.7711e-01, -1.6306e+00],
         [ 8.0550e+00, -1.0849e-02,  1.8769e+01,  ..., -1.1959e-02,
          -2.7712e-01, -1.6306e+00],
         ...,
         [ 8.0553e+00, -1.0973e-02,  1.8769e+01,  ..., -1.1836e-02,
          -2.7690e-01, -1.6303e+00],
         [ 8.0553e+00, -1.1001e-02,  1.8769e+01,  ..., -1.1829e-02,
          -2.7689e-01, -1.6304e+00],
         [ 8.0553e+00, -1.1013e-02,  1.8769e+01,  ..., -1.1876e-02,
          -2.7687e-01, -1.6304e+00]],

        [[ 8.0509e+00, -1.1044e-02,  1.8773e+01,  ..., -1.2656e-02,
          -2.7581e-01, -1.6344e+00],
         [ 8.0510e+00, -1.1074e-02,  1.8773e+01,  ..., -1.2565e-02,
          -2.7584e-01, -1.6345e+00],
         [ 8.0514e+00, -1.1133e-02,  1.8773e+01,  ..., -1.2499e-02,
          -2.7596e-01, -1.6344e+00],
         ...,
         [ 8.0529e+00, -1.1212e-02,  1.8772e+01,  ..., -1.2625e-02,
          -2.7645e-01, -1.6338e+00],
         [ 8.0530e+00, -1.1251e-02,  1.8772e+01,  ..., -1.2567e-02,
          -2.7646e-01, -1.6338e+00],
         [ 8.0531e+00, -1.1276e-02,  1.8772e+01,  ..., -1.2549e-02,
          -2.7648e-01, -1.6338e+00]],

        [[ 8.0540e+00, -1.0309e-02,  1.8768e+01,  ..., -1.1822e-02,
          -2.7697e-01, -1.6288e+00],
         [ 8.0540e+00, -1.0324e-02,  1.8768e+01,  ..., -1.1753e-02,
          -2.7695e-01, -1.6289e+00],
         [ 8.0540e+00, -1.0347e-02,  1.8768e+01,  ..., -1.1680e-02,
          -2.7692e-01, -1.6289e+00],
         ...,
         [ 8.0542e+00, -1.0401e-02,  1.8768e+01,  ..., -1.1565e-02,
          -2.7674e-01, -1.6288e+00],
         [ 8.0541e+00, -1.0429e-02,  1.8768e+01,  ..., -1.1492e-02,
          -2.7671e-01, -1.6288e+00],
         [ 8.0541e+00, -1.0460e-02,  1.8768e+01,  ..., -1.1438e-02,
          -2.7670e-01, -1.6289e+00]],

        ...,

        [[ 8.0554e+00, -1.1082e-02,  1.8770e+01,  ..., -1.1963e-02,
          -2.7765e-01, -1.6323e+00],
         [ 8.0554e+00, -1.1119e-02,  1.8770e+01,  ..., -1.1902e-02,
          -2.7762e-01, -1.6324e+00],
         [ 8.0554e+00, -1.1160e-02,  1.8770e+01,  ..., -1.1836e-02,
          -2.7758e-01, -1.6324e+00],
         ...,
         [ 8.0547e+00, -1.0909e-02,  1.8770e+01,  ..., -1.1936e-02,
          -2.7749e-01, -1.6315e+00],
         [ 8.0555e+00, -1.1037e-02,  1.8770e+01,  ..., -1.1924e-02,
          -2.7758e-01, -1.6318e+00],
         [ 8.0559e+00, -1.1117e-02,  1.8770e+01,  ..., -1.1901e-02,
          -2.7762e-01, -1.6319e+00]],

        [[ 8.0511e+00, -1.1333e-02,  1.8775e+01,  ..., -1.2454e-02,
          -2.7614e-01, -1.6355e+00],
         [ 8.0514e+00, -1.1384e-02,  1.8775e+01,  ..., -1.2386e-02,
          -2.7625e-01, -1.6355e+00],
         [ 8.0519e+00, -1.1453e-02,  1.8774e+01,  ..., -1.2390e-02,
          -2.7634e-01, -1.6354e+00],
         ...,
         [ 8.0526e+00, -1.1487e-02,  1.8774e+01,  ..., -1.2454e-02,
          -2.7659e-01, -1.6351e+00],
         [ 8.0527e+00, -1.1516e-02,  1.8774e+01,  ..., -1.2440e-02,
          -2.7661e-01, -1.6351e+00],
         [ 8.0527e+00, -1.1529e-02,  1.8774e+01,  ..., -1.2477e-02,
          -2.7661e-01, -1.6351e+00]],

        [[ 8.0537e+00, -1.0747e-02,  1.8770e+01,  ..., -1.1981e-02,
          -2.7678e-01, -1.6312e+00],
         [ 8.0537e+00, -1.0778e-02,  1.8770e+01,  ..., -1.1971e-02,
          -2.7678e-01, -1.6312e+00],
         [ 8.0538e+00, -1.0789e-02,  1.8770e+01,  ..., -1.2011e-02,
          -2.7678e-01, -1.6312e+00],
         ...,
         [ 8.0539e+00, -1.0863e-02,  1.8769e+01,  ..., -1.1788e-02,
          -2.7657e-01, -1.6311e+00],
         [ 8.0540e+00, -1.0873e-02,  1.8769e+01,  ..., -1.1827e-02,
          -2.7656e-01, -1.6310e+00],
         [ 8.0540e+00, -1.0863e-02,  1.8769e+01,  ..., -1.1896e-02,
          -2.7654e-01, -1.6310e+00]]], device='cuda:0')
torch.Size([64, 4, 16, 768])
torch.Size([4, 16, 768])
Iteration: 1
time taken for epoch 0:01:34.923892
Total pretraining time 0:01:34.924002
